{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643bd78a-4e66-4231-9090-53855eb49559",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3d0c00-26c4-47fd-8cdb-05c7488e56ca",
   "metadata": {},
   "source": [
    "**Lasso Regression (Least Absolute Shrinkage and Selection Operator):**\n",
    "\n",
    "Lasso Regression is a linear regression technique that combines the principles of ordinary least squares (OLS) regression with a regularization term. The key characteristic of Lasso Regression is its ability to perform variable selection by setting some regression coefficients exactly to zero. This feature makes Lasso particularly useful in situations with high-dimensional data or when there is a suspicion that many predictors are irrelevant.\n",
    "\n",
    "**Objective Function of Lasso Regression:**\n",
    "\\[ \\text{Minimize} \\left\\{ \\text{Sum of Squared Residuals} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} \\]\n",
    "\n",
    "- **Sum of Squared Residuals:** Similar to OLS, this term represents the difference between the predicted and actual values.\n",
    "\n",
    "- **Regularization Term (\\(\\lambda \\sum_{j=1}^{p} |\\beta_j|\\)):** This term penalizes the sum of the absolute values of the coefficients (\\(\\beta_j\\)). The hyperparameter \\(\\lambda\\) controls the strength of the penalty. The larger the \\(\\lambda\\), the stronger the regularization, and the more the coefficients are pushed towards zero.\n",
    "\n",
    "**Key Differences from Other Regression Techniques:**\n",
    "\n",
    "1. **Variable Selection:**\n",
    "   - One of the main advantages of Lasso Regression is its ability to perform automatic variable selection. As the regularization parameter (\\(\\lambda\\)) increases, Lasso tends to shrink some coefficients exactly to zero, effectively removing the corresponding predictors from the model. This feature is particularly valuable in high-dimensional datasets with many irrelevant features.\n",
    "\n",
    "2. **Sparse Solutions:**\n",
    "   - Lasso tends to produce sparse solutions, meaning that only a subset of the predictors has non-zero coefficients. This sparsity makes the model more interpretable and can lead to more efficient and concise models.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - The Lasso penalty has a stronger impact on coefficient estimates compared to Ridge Regression. In Ridge, coefficients are shrunk towards zero, but rarely set exactly to zero. In Lasso, some coefficients can be exactly zero, resulting in a more parsimonious model.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - While Lasso can perform well in the presence of multicollinearity, it tends to arbitrarily choose one variable from a group of highly correlated variables and set the coefficients of the others to zero. This can be an advantage or disadvantage depending on the context.\n",
    "\n",
    "5. **Shape of Constraint Region:**\n",
    "   - The shape of the constraint region defined by the regularization term in Lasso is different from that in Ridge Regression. In Lasso, the constraint region has corners at the axes, which contributes to the sparse solution.\n",
    "\n",
    "6. **Sensitivity to Outliers:**\n",
    "   - Lasso can be sensitive to outliers in the data, and the presence of outliers might disproportionately influence the coefficient estimates.\n",
    "\n",
    "In summary, Lasso Regression is a linear regression technique with a regularization term that promotes sparsity in the model by setting some coefficients exactly to zero. This makes it a valuable tool for feature selection and model simplification, especially in scenarios with a large number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b890dc7-e883-47e7-87de-fe6075cf9f69",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ee505e-80ef-48eb-a9ce-fe30ba880d14",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection lies in its ability to automatically select a subset of relevant features by setting some of the regression coefficients exactly to zero. This feature selection property addresses the challenge of high-dimensional data, where the number of predictors is much larger than the number of observations.\n",
    "\n",
    "Here are key advantages of Lasso Regression in feature selection:\n",
    "\n",
    "1. **Automatic Variable Selection:**\n",
    "   - Lasso Regression automatically selects a subset of predictors by shrinking some coefficients to exactly zero. This means that it identifies and retains only the most important predictors for the task at hand.\n",
    "\n",
    "2. **Sparse Solutions:**\n",
    "   - Lasso tends to produce sparse solutions, where a significant number of coefficients are exactly zero. This sparsity makes the model more interpretable and efficient, particularly when dealing with datasets with a large number of potentially irrelevant features.\n",
    "\n",
    "3. **Model Simplicity:**\n",
    "   - By forcing some coefficients to zero, Lasso simplifies the model and reduces overfitting. The resulting model is more parsimonious and less prone to capturing noise or irrelevant information in the data.\n",
    "\n",
    "4. **Improved Generalization:**\n",
    "   - The sparsity induced by Lasso often leads to improved generalization performance on new, unseen data. The model is less likely to be overly complex and may have enhanced predictive power.\n",
    "\n",
    "5. **Enhanced Interpretability:**\n",
    "   - A Lasso model with sparse coefficients is easier to interpret, making it more accessible to practitioners and stakeholders. It provides insights into which variables are deemed important for the task and which can be safely excluded.\n",
    "\n",
    "6. **Handling Multicollinearity:**\n",
    "   - Lasso can handle multicollinearity by selecting one variable from a group of highly correlated variables and setting the coefficients of the others to zero. This can be advantageous in situations where variable redundancy is a concern.\n",
    "\n",
    "7. **Feature Engineering Aid:**\n",
    "   - Lasso Regression can be used as a tool for feature engineering, helping to identify and retain the most relevant features. This is particularly useful when dealing with datasets containing a mix of informative and irrelevant predictors.\n",
    "\n",
    "8. **Regularization Control:**\n",
    "   - The strength of the regularization in Lasso is controlled by the hyperparameter \\(\\lambda\\). By tuning \\(\\lambda\\), practitioners can adjust the trade-off between fitting the data and enforcing sparsity, allowing flexibility in the level of feature selection.\n",
    "\n",
    "In summary, the main advantage of Lasso Regression in feature selection is its capability to automatically identify and retain a subset of important predictors, leading to simpler and more interpretable models, improved generalization performance, and enhanced efficiency in high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c842e9-f011-4322-96fb-6e3d93f882f1",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451c940-96f2-4eb6-926b-6a458c0f1ea1",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting the coefficients in ordinary least squares (OLS) regression, but with the added consideration of the regularization effect. Here are key points to keep in mind when interpreting the coefficients of a Lasso Regression model:\n",
    "\n",
    "1. **Magnitude and Sign of Coefficients:**\n",
    "   - As in OLS regression, the sign of the coefficients indicates the direction of the relationship between each predictor variable and the response variable. A positive coefficient implies a positive relationship, while a negative coefficient implies a negative relationship.\n",
    "\n",
    "   - The magnitude of the coefficients is influenced by both the strength of the relationship with the response variable and the regularization term. In Lasso Regression, some coefficients may be exactly zero due to the sparsity-inducing property of Lasso.\n",
    "\n",
    "2. **Zero Coefficients:**\n",
    "   - Lasso Regression has the ability to set some coefficients exactly to zero. A coefficient being zero means that the corresponding predictor variable is effectively excluded from the model. This feature provides automatic variable selection, and the non-zero coefficients indicate the retained features.\n",
    "\n",
    "3. **Feature Importance:**\n",
    "   - The non-zero coefficients in a Lasso model indicate the importance of the corresponding features. Larger absolute values suggest a stronger impact on the response variable.\n",
    "\n",
    "4. **Comparisons of Coefficients:**\n",
    "   - The magnitude of the coefficients can be compared to assess the relative importance of different predictors. However, it's important to consider the scale of the predictors, as Lasso is sensitive to the scale of variables.\n",
    "\n",
    "5. **Trade-off Between Bias and Variance:**\n",
    "   - Lasso Regression introduces a trade-off between bias and variance. The regularization term shrinks coefficients towards zero, reducing variance but introducing some bias. The choice of the regularization parameter (\\(\\lambda\\)) determines the strength of this trade-off.\n",
    "\n",
    "6. **Effect of Multicollinearity:**\n",
    "   - Lasso can handle multicollinearity by choosing one variable from a group of highly correlated variables and setting the coefficients of the others to zero. This can affect the interpretation of coefficients and requires careful consideration.\n",
    "\n",
    "7. **Standardization for Fair Comparison:**\n",
    "   - To facilitate fair comparisons between coefficients, it's common to standardize the predictor variables before applying Lasso Regression. Standardization involves subtracting the mean and dividing by the standard deviation for each variable.\n",
    "\n",
    "8. **Interpretation of Interaction Terms:**\n",
    "   - If interaction terms are included in the model, the interpretation becomes more complex. The impact of one variable on the response depends on the values of other interacting variables.\n",
    "\n",
    "In summary, interpreting the coefficients of a Lasso Regression model involves considering the direction, magnitude, and sparsity of the coefficients. The sparsity property provides valuable insights into variable selection and simplification of the model. Understanding the trade-off introduced by the regularization term is crucial for proper interpretation and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36a3936-f407-4e18-9f0f-816dd572bbdc",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815931f-edb8-4914-b6e6-dea3c33d3b6a",
   "metadata": {},
   "source": [
    "In Lasso Regression, the main tuning parameter is the regularization parameter, often denoted as \\(\\lambda\\) (lambda). This parameter controls the strength of the regularization penalty applied to the coefficients. The objective function of Lasso Regression includes a penalty term proportional to the sum of the absolute values of the coefficients:\n",
    "\n",
    "\\[ \\text{Minimize} \\left\\{ \\text{Sum of Squared Residuals} + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right\\} \\]\n",
    "\n",
    "Here, \\(\\lambda\\) is the regularization parameter, and \\(|\\beta_j|\\) represents the absolute value of the coefficients.\n",
    "\n",
    "The impact of the regularization parameter on Lasso Regression can be summarized as follows:\n",
    "\n",
    "1. **Low \\(\\lambda\\):**\n",
    "   - When \\(\\lambda\\) is close to zero, the penalty term has minimal impact. The model behaves more like ordinary least squares (OLS) regression, and all features are retained. The coefficients are not strongly penalized, allowing for a more complex model.\n",
    "\n",
    "2. **Intermediate \\(\\lambda\\):**\n",
    "   - As \\(\\lambda\\) increases, the penalty term becomes more influential. Some coefficients start getting pushed towards zero, leading to sparsity in the model. The model becomes simpler, and irrelevant features may have their coefficients set to exactly zero, effectively excluding them.\n",
    "\n",
    "3. **High \\(\\lambda\\):**\n",
    "   - When \\(\\lambda\\) is large, the penalty term dominates the objective function. Many coefficients are pushed towards zero, resulting in a highly sparse model. The model is simpler, and only a subset of the most important features is retained.\n",
    "\n",
    "The choice of \\(\\lambda\\) is critical, and it is often determined through techniques such as cross-validation. Cross-validation involves training and evaluating the model on different subsets of the data to find the \\(\\lambda\\) that provides the best balance between model complexity and performance.\n",
    "\n",
    "In addition to \\(\\lambda\\), some implementations of Lasso Regression may also allow for alternative formulations of the penalty term, such as the elastic net penalty, which combines both L1 (lasso) and L2 (ridge) penalties. The elastic net introduces an additional parameter (\\(\\alpha\\)) to control the mixing between the L1 and L2 penalties. When \\(\\alpha = 1\\), it is equivalent to Lasso Regression, while \\(\\alpha = 0\\) is equivalent to Ridge Regression. Intermediate values of \\(\\alpha\\) allow for a combination of L1 and L2 regularization.\n",
    "\n",
    "In summary, the tuning parameters in Lasso Regression are primarily the regularization parameter (\\(\\lambda\\)) and, optionally, the mixing parameter (\\(\\alpha\\)) in the case of elastic net regularization. Adjusting these parameters influences the model's complexity, sparsity, and performance, and the optimal values are often determined through techniques like cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c578318a-441a-445b-a962-f57e49867e0a",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c85804-3bf9-49bc-9ee9-9adac0ad7d69",
   "metadata": {},
   "source": [
    "Lasso Regression, by itself, is a linear regression technique, and it's designed for linear relationships between predictors and the response variable. However, it is possible to adapt Lasso Regression for non-linear regression problems by incorporating non-linear transformations of the features. This approach is sometimes referred to as \"Lasso with non-linear features\" or \"Non-linear Lasso.\"\n",
    "\n",
    "Here's how you can use Lasso Regression for non-linear regression:\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Create non-linear transformations of the predictor variables. Common transformations include squaring the variables (\\(x^2\\)), taking square roots, logarithmic transformations, or any other non-linear functions.\n",
    "\n",
    "2. **Apply Lasso Regression:**\n",
    "   - Use the original and non-linearly transformed features as input to the Lasso Regression model.\n",
    "\n",
    "3. **Regularization:**\n",
    "   - Apply Lasso's regularization, which can help with feature selection and prevent overfitting, even in the presence of non-linearities.\n",
    "\n",
    "4. **Tuning Parameters:**\n",
    "   - Tune the regularization parameter (\\(\\lambda\\)) appropriately. The choice of \\(\\lambda\\) will affect the degree of regularization and, consequently, the model's ability to capture non-linear relationships.\n",
    "\n",
    "5. **Cross-Validation:**\n",
    "   - Utilize cross-validation to find the optimal values of \\(\\lambda\\) and other hyperparameters, ensuring the model generalizes well to unseen data.\n",
    "\n",
    "6. **Interaction Terms:**\n",
    "   - Include interaction terms between variables if needed. Interaction terms capture combined effects of predictors and can help model more complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe953b7-a85d-41cc-9899-1fad1c388799",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbc652a-c081-46f1-b8d8-a95a0f92e240",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both linear regression techniques with regularization, but they differ in how they apply the regularization term to the regression coefficients. Here are the main differences between Ridge Regression and Lasso Regression:\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - **Ridge Regression:** The regularization term in Ridge Regression is the sum of the squared values of the coefficients, multiplied by a regularization parameter (\\(\\lambda\\)). It is expressed as \\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\), where \\(\\beta_j\\) represents the regression coefficients.\n",
    "\n",
    "   - **Lasso Regression:** The regularization term in Lasso Regression is the sum of the absolute values of the coefficients, multiplied by a regularization parameter (\\(\\lambda\\)). It is expressed as \\(\\lambda \\sum_{j=1}^{p} |\\beta_j|\\).\n",
    "\n",
    "2. **Sparsity:**\n",
    "   - **Ridge Regression:** Ridge Regression does not set coefficients exactly to zero. It shrinks the coefficients towards zero, but they rarely become exactly zero. Ridge tends to retain all features but with smaller magnitudes.\n",
    "\n",
    "   - **Lasso Regression:** Lasso Regression has the property of variable selection. As the regularization parameter increases, some coefficients are exactly set to zero. Lasso tends to produce sparse models, where only a subset of features is retained.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - **Ridge Regression:** The regularization term in Ridge Regression adds a penalty based on the square of each coefficient. It results in a shrinkage of coefficients towards zero, but they remain non-zero.\n",
    "\n",
    "   - **Lasso Regression:** The Lasso penalty, based on the absolute values of coefficients, encourages sparsity. Some coefficients become exactly zero, leading to feature selection.\n",
    "\n",
    "4. **Handling Multicollinearity:**\n",
    "   - **Ridge Regression:** Ridge Regression is effective in handling multicollinearity by distributing the impact of correlated predictors across them. It doesn't eliminate any variable entirely.\n",
    "\n",
    "   - **Lasso Regression:** Lasso Regression can be more arbitrary in the presence of multicollinearity. It tends to arbitrarily choose one variable from a group of highly correlated variables and sets the coefficients of the others to zero.\n",
    "\n",
    "5. **Geometric Interpretation:**\n",
    "   - **Ridge Regression:** In a geometric sense, Ridge Regression can be visualized as adding a \"ridge\" to the contour plot of the least squares cost function. The ridge prevents the coefficients from reaching the origin.\n",
    "\n",
    "   - **Lasso Regression:** Lasso Regression introduces a diamond-shaped constraint in the contour plot, leading to some coefficients reaching exactly zero at the corners.\n",
    "\n",
    "6. **Equation Systems:**\n",
    "   - **Ridge Regression:** The optimization problem involves minimizing the sum of squared residuals and the squared values of the coefficients.\n",
    "\n",
    "   - **Lasso Regression:** The optimization problem involves minimizing the sum of squared residuals and the absolute values of the coefficients.\n",
    "\n",
    "In summary, the key distinction between Ridge Regression and Lasso Regression is in the shape of the regularization term. Ridge uses the squared values of coefficients, encouraging shrinkage but not sparsity. Lasso uses the absolute values of coefficients, leading to sparsity and feature selection. The choice between them depends on the specific goals of the analysis and the characteristics of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c60036-13e3-4482-bdf2-4f6f8e51572b",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4689409-a0fc-4255-a906-edd5af493aaa",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression has the ability to handle multicollinearity in input features, and it does so through its inherent feature selection property. Multicollinearity arises when two or more predictor variables in a regression model are highly correlated, making it challenging to estimate individual coefficients accurately. Lasso Regression addresses multicollinearity by automatically selecting a subset of relevant features and setting the coefficients of some features to exactly zero.\n",
    "\n",
    "Here's how Lasso Regression handles multicollinearity:\n",
    "\n",
    "1. **Feature Selection:**\n",
    "   - Lasso Regression introduces a penalty term in its objective function, proportional to the sum of the absolute values of the coefficients. As the regularization parameter (\\(\\lambda\\)) increases, Lasso encourages sparsity in the model. This means that some coefficients become exactly zero, effectively excluding the corresponding features from the model.\n",
    "\n",
    "2. **Arbitrary Selection:**\n",
    "   - In the presence of multicollinearity, Lasso tends to arbitrarily choose one variable from a group of highly correlated variables and sets the coefficients of the others to zero. The choice of which variable to keep depends on the specifics of the optimization process.\n",
    "\n",
    "3. **Reduction of Dimensionality:**\n",
    "   - By setting some coefficients to zero, Lasso effectively reduces the dimensionality of the model. This reduction is particularly useful in situations where the number of predictors is large compared to the number of observations.\n",
    "\n",
    "4. **Stability of Coefficient Estimates:**\n",
    "   - Lasso's feature selection helps stabilize the coefficient estimates by focusing on a subset of features. This can be beneficial when multicollinearity causes instability in coefficient estimates in traditional regression models.\n",
    "\n",
    "5. **Trade-off Between Bias and Variance:**\n",
    "   - The regularization term in Lasso introduces a trade-off between bias and variance. By encouraging sparsity, Lasso sacrifices the flexibility to fit the data precisely in exchange for a simpler and more interpretable model. This trade-off helps mitigate the impact of multicollinearity.\n",
    "\n",
    "6. **Handling of Near-Collinear Features:**\n",
    "   - Lasso can handle not only perfect multicollinearity (where one variable is a perfect linear combination of others) but also near-collinearity. It may distribute the impact of correlated variables across them or exclude some of them, depending on the optimization process.\n",
    "\n",
    "It's important to note that while Lasso Regression can be effective in addressing multicollinearity, it does have limitations. In situations where preserving all variables is essential, or when the choice of which variable to keep is critical, other techniques such as Ridge Regression or methods specifically designed for dealing with multicollinearity might be considered. Additionally, the choice of the regularization parameter (\\(\\lambda\\)) in Lasso should be carefully tuned, often through cross-validation, to achieve the desired level of sparsity and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355fea01-0607-4ca5-87b4-0fbc4694274e",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a3a42d-2e56-4c7c-ab36-188c1db2a586",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression is a crucial step in ensuring the model's performance and achieving the right balance between fitting the data and preventing overfitting. Commonly, cross-validation is used to assess the model's performance under different values of \\(\\lambda\\) and select the optimal one. Here are the steps typically followed:\n",
    "\n",
    "1. **Grid Search:**\n",
    "   - Define a range or grid of possible \\(\\lambda\\) values to explore. This is often done on a logarithmic scale to cover a broad range, such as \\(\\lambda = 10^{-5}, 10^{-4}, \\ldots, 10^3, 10^4\\).\n",
    "\n",
    "2. **Cross-Validation:**\n",
    "   - Split the dataset into training and validation sets. Typically, k-fold cross-validation is used, where the data is divided into k subsets (folds), and the model is trained and evaluated k times, each time using a different fold as the validation set.\n",
    "\n",
    "3. **Model Training:**\n",
    "   - For each \\(\\lambda\\) value, train the Lasso Regression model on the training data. The model is trained using the training set, and its performance is evaluated on the validation set.\n",
    "\n",
    "4. **Performance Metric:**\n",
    "   - Choose a performance metric to evaluate the model. Common metrics for regression problems include Mean Squared Error (MSE), Mean Absolute Error (MAE), or \\(R^2\\) (coefficient of determination).\n",
    "\n",
    "5. **Select Optimal \\(\\lambda\\):**\n",
    "   - Choose the \\(\\lambda\\) value that minimizes the chosen performance metric across all folds. For example, if using MSE, select the \\(\\lambda\\) that gives the lowest average MSE across the folds.\n",
    "\n",
    "6. **Train Final Model:**\n",
    "   - Once the optimal \\(\\lambda\\) is determined, train the final Lasso Regression model using the entire training dataset with the chosen \\(\\lambda\\).\n",
    "\n",
    "7. **Evaluate on Test Set:**\n",
    "   - Evaluate the final model on a separate test set to assess its generalization performance on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551406d-07c6-4f37-97fc-e5fb2103af2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
