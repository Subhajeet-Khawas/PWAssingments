{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eafd87f-aea7-4cea-8ce0-b80f1f0e3620",
   "metadata": {},
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6360be-558e-4e46-a473-b7c3bc7bba41",
   "metadata": {},
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It's named after the Reverend Thomas Bayes, who introduced the theorem. The formula for Bayes' Theorem is as follows:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Here's what each term represents:\n",
    "\n",
    "- \\( P(A|B) \\): The probability of event A occurring given that event B has occurred. This is called the posterior probability.\n",
    "- \\( P(B|A) \\): The probability of event B occurring given that event A has occurred. This is called the likelihood.\n",
    "- \\( P(A) \\): The prior probability of event A, i.e., the probability of A occurring without any knowledge of B.\n",
    "- \\( P(B) \\): The prior probability of event B, i.e., the probability of B occurring without any knowledge of A.\n",
    "\n",
    "Bayes' Theorem is often used in a variety of fields, including statistics, machine learning, and artificial intelligence. It allows us to update probabilities based on new evidence or information, making it a powerful tool for reasoning under uncertainty.\n",
    "\n",
    "The theorem is particularly important in Bayesian statistics, where it serves as a foundation for Bayesian inference, a method for updating probabilities based on new data. It has applications in various areas, such as spam filtering, medical diagnosis, and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d4b0f-414b-418c-8133-eca04080a066",
   "metadata": {},
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9134a-7251-4571-974f-97abe3d7b142",
   "metadata": {},
   "source": [
    "Bayes' Theorem is mathematically expressed as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Here's the breakdown of each term in the formula:\n",
    "\n",
    "- \\( P(A|B) \\): The posterior probability of event A given that event B has occurred. This is what we want to calculate.\n",
    "- \\( P(B|A) \\): The likelihood or conditional probability of event B given that event A has occurred.\n",
    "- \\( P(A) \\): The prior probability of event A, i.e., the probability of A occurring without any knowledge of B.\n",
    "- \\( P(B) \\): The prior probability of event B, i.e., the probability of B occurring without any knowledge of A.\n",
    "\n",
    "Bayes' Theorem is a fundamental concept in probability theory and provides a way to update probabilities based on new evidence or information. It is widely used in various fields, including statistics, machine learning, and artificial intelligence, for making informed decisions under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d973ba-510e-4137-b1da-ec9b9979688e",
   "metadata": {},
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39922328-7a00-49ec-b74b-274a4b4793e4",
   "metadata": {},
   "source": [
    "Bayes' Theorem is used in various fields to make probabilistic predictions or decisions based on new evidence or information. Its practical applications span several domains, and here are a few examples:\n",
    "\n",
    "1. **Medical Diagnosis:**\n",
    "   - In medical diagnosis, Bayes' Theorem is used to update the probability of a disease given certain symptoms or test results.\n",
    "   - For instance, if a patient has symptoms that are indicative of a particular condition, the theorem helps adjust the probability of that condition based on the patient's specific symptoms and medical history.\n",
    "\n",
    "2. **Spam Filtering:**\n",
    "   - Bayes' Theorem is employed in spam filtering algorithms to determine the probability that an incoming email is spam given certain features (words, patterns, etc.).\n",
    "   - The algorithm learns from historical data and updates its predictions based on the occurrence of certain words in emails.\n",
    "\n",
    "3. **Machine Learning and Classification:**\n",
    "   - In machine learning, particularly in Naive Bayes classifiers, Bayes' Theorem is used for classification tasks.\n",
    "   - Given features of an input (e.g., words in a document), the theorem helps calculate the probability of belonging to a particular class.\n",
    "\n",
    "4. **A/B Testing:**\n",
    "   - In statistical hypothesis testing, Bayes' Theorem is used to update the probability of a hypothesis given observed data.\n",
    "   - For example, in A/B testing, it can be used to update the probability that a variation is better than another based on user interactions.\n",
    "\n",
    "5. **Fault Diagnosis:**\n",
    "   - In fault diagnosis systems, Bayes' Theorem helps update the probability of a particular fault given observed symptoms or sensor readings.\n",
    "   - It's commonly used in scenarios like detecting faults in industrial equipment.\n",
    "\n",
    "6. **Weather Forecasting:**\n",
    "   - In weather forecasting, Bayes' Theorem is used to update the probability of certain weather conditions based on new observations.\n",
    "   - It helps meteorologists refine predictions as more data becomes available.\n",
    "\n",
    "7. **Document Classification:**\n",
    "   - In natural language processing, Bayes' Theorem is used for document classification tasks.\n",
    "   - Given the words in a document, the theorem helps calculate the probability of the document belonging to a certain category (e.g., topic or sentiment).\n",
    "\n",
    "In each of these applications, Bayes' Theorem provides a principled way to incorporate prior knowledge (prior probabilities) with new evidence (likelihoods) to arrive at updated probabilities or decisions. The theorem is a key tool for reasoning under uncertainty and updating beliefs as more information becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0e6388-0e5c-4ed0-8de0-dcb852ca8a06",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57da28-f20e-47ab-820d-149def4ef92a",
   "metadata": {},
   "source": [
    "Bayes' Theorem is closely related to conditional probability and can be derived from the definition of conditional probability. Let's explore the relationship between Bayes' Theorem and conditional probability.\n",
    "\n",
    "### Conditional Probability:\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. Mathematically, the conditional probability of event A given event B is denoted as \\( P(A|B) \\) and is calculated as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\]\n",
    "\n",
    "Here:\n",
    "- \\( P(A \\cap B) \\) is the probability of both events A and B occurring.\n",
    "- \\( P(B) \\) is the probability of event B occurring.\n",
    "\n",
    "### Bayes' Theorem:\n",
    "Bayes' Theorem provides a way to reverse the conditioning and calculate the probability of the reverse event. It is expressed as:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "Here:\n",
    "- \\( P(A|B) \\) is the posterior probability of event A given that event B has occurred.\n",
    "- \\( P(B|A) \\) is the likelihood or conditional probability of event B given that event A has occurred.\n",
    "- \\( P(A) \\) is the prior probability of event A.\n",
    "- \\( P(B) \\) is the prior probability of event B.\n",
    "\n",
    "### Relationship:\n",
    "By comparing the two formulas, you can observe the relationship:\n",
    "\n",
    "\\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\]\n",
    "\n",
    "- The numerator \\( P(B|A) \\cdot P(A) \\) corresponds to \\( P(A \\cap B) \\), the probability of both events A and B occurring.\n",
    "- The denominator \\( P(B) \\) corresponds to \\( P(B) \\), the probability of event B occurring.\n",
    "\n",
    "In essence, Bayes' Theorem provides a formula for updating the probability of event A given new evidence (event B). It links the conditional probability \\( P(A|B) \\) to the likelihood \\( P(B|A) \\), the prior probability \\( P(A) \\), and the marginal probability \\( P(B) \\).\n",
    "\n",
    "Bayes' Theorem is a powerful tool for updating beliefs or probabilities based on new information, and it is derived from the principles of conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb54c11-c9c5-479b-92f5-97632116295d",
   "metadata": {},
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2a363-d379-43e1-9233-f5fb9f03245c",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier depends on the characteristics of your data and the assumptions you are willing to make about the independence of features. There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's a guide on how to choose:\n",
    "\n",
    "### 1. Gaussian Naive Bayes:\n",
    "- **Data Type:** Suitable for continuous or numerical features that can be modeled using a Gaussian (normal) distribution.\n",
    "- **Example Applications:**\n",
    "  - Natural language processing tasks when considering word frequencies.\n",
    "  - Predictive modeling with real-valued features.\n",
    "\n",
    "### 2. Multinomial Naive Bayes:\n",
    "- **Data Type:** Suitable for discrete features, typically counts or frequencies of events in a fixed-size sample.\n",
    "- **Example Applications:**\n",
    "  - Text classification problems where features represent word counts (bag-of-words models).\n",
    "  - Document classification, spam filtering, and sentiment analysis.\n",
    "\n",
    "### 3. Bernoulli Naive Bayes:\n",
    "- **Data Type:** Suitable for binary or Boolean features, where each feature is either present or absent.\n",
    "- **Example Applications:**\n",
    "  - Text classification problems when considering the presence or absence of words (binary document representation).\n",
    "  - Spam filtering using binary features indicating the presence of certain words.\n",
    "\n",
    "### Choosing the Right Naive Bayes Classifier:\n",
    "1. **Nature of Features:**\n",
    "   - **Continuous Features:** Use Gaussian Naive Bayes.\n",
    "   - **Discrete Counts or Frequencies:** Use Multinomial Naive Bayes.\n",
    "   - **Binary Features (0/1):** Use Bernoulli Naive Bayes.\n",
    "\n",
    "2. **Assumptions:**\n",
    "   - **Gaussian Naive Bayes:** Assumes features are normally distributed.\n",
    "   - **Multinomial Naive Bayes:** Assumes features are counts or frequencies.\n",
    "   - **Bernoulli Naive Bayes:** Assumes binary features and is suitable for binary data.\n",
    "\n",
    "3. **Size of Dataset:**\n",
    "   - **Gaussian Naive Bayes:** Generally robust with small datasets.\n",
    "   - **Multinomial and Bernoulli Naive Bayes:** Can work well with larger datasets.\n",
    "\n",
    "4. **Performance Considerations:**\n",
    "   - **Gaussian Naive Bayes:** May perform well when continuous features are approximately normally distributed.\n",
    "   - **Multinomial and Bernoulli Naive Bayes:** May perform well for discrete or binary data.\n",
    "\n",
    "5. **Domain Knowledge:**\n",
    "   - Consider the nature of your problem and whether the assumptions of each type align with your understanding of the data.\n",
    "\n",
    "It's important to note that despite the \"naive\" assumption of independence among features, Naive Bayes classifiers can perform surprisingly well in practice, especially when the independence assumption is not severely violated. Experimentation and validation on your specific dataset are key to determining the most suitable Naive Bayes classifier for your problem. Cross-validation and model evaluation techniques help in assessing the performance of different Naive Bayes variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d7ad9-c6ba-4618-855b-540a2b7a5b36",
   "metadata": {},
   "source": [
    "Q6. Assignment:\n",
    "\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency ofeach feature value for each class:\n",
    "\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "\n",
    "A 3 3 4 4 3 3 3\n",
    "\n",
    "B 2 2 1 2 2 2 3\n",
    "\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740cc08-937a-42b3-908c-a2c80710d673",
   "metadata": {},
   "source": [
    "To solve this problem using Naive Bayes, we'll calculate the posterior probability for each class and then choose the class with the highest posterior probability. The posterior probability can be calculated using the formula:\n",
    "\n",
    "$$ P(Class|X1,X2) = \\frac{P(X1|Class) \\cdot P(X2|Class) \\cdot P(Class)}{P(X1) \\cdot P(X2)} $$\n",
    "\n",
    "However, since we are assuming equal prior probabilities for each class and the denominator is constant for all classes, we can ignore it and calculate the numerator only. The class with the highest numerator will be the predicted class.\n",
    "\n",
    "Here's the plan of execution:\n",
    "- Calculate the likelihood of X1=3 and X2=4 for Class A.\n",
    "- Calculate the likelihood of X1=3 and X2=4 for Class B.\n",
    "- Compare the likelihoods and choose the class with the highest value.\n",
    "\n",
    "Let's start by calculating the likelihood for Class A and Class B.<FUNCTION_CALL />\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Class</th>\n",
    "      <th>P(X1=3|Class)</th>\n",
    "      <th>P(X2=4|Class)</th>\n",
    "      <th>Likelihood</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>A</td>\n",
    "      <td>0.4</td>\n",
    "      <td>0.23076923076923078</td>\n",
    "      <td>0.09230769230769231</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>B</td>\n",
    "      <td>0.2</td>\n",
    "      <td>0.3333333333333333</td>\n",
    "      <td>0.06666666666666667</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "Based on the calculated likelihoods, Class A has a higher likelihood than Class B for the new instance with features X1 = 3 and X2 = 4. Therefore, Naive Bayes would predict that the new instance belongs to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b99b52d-fd76-415c-b7ed-77c8400f3f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Class  P(X1=3|Class)  P(X2=4|Class)  Likelihood\n",
      "0     A            0.4       0.230769    0.092308\n",
      "1     B            0.2       0.333333    0.066667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class_data = {\n",
    "    'Class': ['A', 'B'],\n",
    "    'X1': [3+3+4, 2+2+1],  # Totals for X1 for A and B\n",
    "    'X2': [4+3+3+3, 2+2+2+3],  # Totals for X2 for A and B\n",
    "    'X1=3': [4, 1],  # Frequency of X1=3 for A and B\n",
    "    'X2=4': [3, 3]  # Frequency of X2=4 for A and B\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(class_data)\n",
    "\n",
    "df['P(X1=3|Class)'] = df['X1=3'] / df['X1']\n",
    "df['P(X2=4|Class)'] = df['X2=4'] / df['X2']\n",
    "\n",
    "df['Likelihood'] = df['P(X1=3|Class)'] * df['P(X2=4|Class)']\n",
    "\n",
    "# Since the prior probabilities are equal, we can ignore them in our calculation.\n",
    "# The class with the highest likelihood will be our prediction.\n",
    "\n",
    "print(df[['Class', 'P(X1=3|Class)', 'P(X2=4|Class)', 'Likelihood']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c8533-86f1-4411-a118-a6a3b47cfc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
