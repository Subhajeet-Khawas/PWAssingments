{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d7a3c8-1799-4e07-a071-d00b98f4f633",
   "metadata": {},
   "source": [
    "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77d032-b5f2-42bc-b07d-22f9a1e06946",
   "metadata": {},
   "source": [
    "**Overfitting:**\n",
    "\n",
    "**Definition:**\n",
    "Overfitting occurs when a machine learning model learns the training data too well, capturing noise or random fluctuations in the data as if they were significant patterns. As a result, the model performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "**Consequences:**\n",
    "1. **Poor Generalization:** The model fails to generalize to new instances, leading to poor performance on unseen data.\n",
    "2. **High Variance:** The model is overly complex, capturing noise in the training data, and may not perform well on different datasets.\n",
    "\n",
    "**Mitigation Techniques:**\n",
    "1. **Cross-Validation:** Use techniques like k-fold cross-validation to assess model performance on different subsets of the training data.\n",
    "2. **Feature Selection:** Select relevant features and eliminate irrelevant ones to reduce model complexity.\n",
    "3. **Regularization:** Apply regularization techniques (e.g., L1 or L2 regularization) to penalize overly complex models.\n",
    "4. **Pruning:** For decision tree-based models, prune the tree to prevent it from becoming too deep and capturing noise.\n",
    "5. **Ensemble Methods:** Use ensemble methods like Random Forest or Gradient Boosting to combine multiple models and reduce overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "**Underfitting:**\n",
    "\n",
    "**Definition:**\n",
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the training data. The model may lack the complexity needed to represent the relationships between features and the target variable accurately.\n",
    "\n",
    "**Consequences:**\n",
    "1. **Ineffective Predictions:** The model fails to capture the underlying patterns, leading to inaccurate predictions on both the training and test data.\n",
    "2. **High Bias:** The model is too simplistic, resulting in high bias and poor performance.\n",
    "\n",
    "**Mitigation Techniques:**\n",
    "1. **Feature Engineering:** Introduce more relevant features to help the model capture complex relationships.\n",
    "2. **Increase Model Complexity:** Use more complex models that can capture intricate patterns in the data.\n",
    "3. **Add Polynomial Features:** For linear models, add polynomial features to allow for more flexibility.\n",
    "4. **Ensemble Methods:** Use ensemble methods to combine multiple models and increase overall model complexity.\n",
    "5. **Hyperparameter Tuning:** Adjust hyperparameters to find a better balance between bias and variance.\n",
    "\n",
    "**Balancing Overfitting and Underfitting:**\n",
    "- Finding the right balance between overfitting and underfitting involves fine-tuning model complexity, selecting appropriate features, and using validation techniques to assess generalization performance.\n",
    "\n",
    "**Regularization Techniques:**\n",
    "- Regularization techniques, such as L1 or L2 regularization, add penalty terms to the loss function, encouraging the model to avoid extreme parameter values and reducing overfitting.\n",
    "\n",
    "**Cross-Validation:**\n",
    "- Cross-validation helps assess a model's performance on different subsets of the data, providing insights into how well the model generalizes to new instances.\n",
    "\n",
    "**Model Evaluation:**\n",
    "- Regularly evaluate a model's performance on both training and test data to ensure it is learning meaningful patterns without capturing noise or being too simplistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9485d7-7767-4510-9311-1b6bb921f193",
   "metadata": {},
   "source": [
    "Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690cf7b0-7d2a-44c1-97a9-71c69a9ae71d",
   "metadata": {},
   "source": [
    "Reducing overfitting is crucial for building machine learning models that generalize well to new, unseen data. Here are some common techniques to reduce overfitting:\n",
    "\n",
    "1. **Cross-Validation:**\n",
    "   - Use techniques like k-fold cross-validation to assess the model's performance on different subsets of the training data. This helps in identifying how well the model generalizes to diverse samples.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Choose relevant features that contribute to the model's performance and eliminate irrelevant ones. Feature selection helps in reducing the complexity of the model and mitigates overfitting.\n",
    "\n",
    "3. **Regularization:**\n",
    "   - Apply regularization techniques, such as L1 or L2 regularization, to penalize extreme parameter values. Regularization adds penalty terms to the loss function, discouraging overly complex models.\n",
    "\n",
    "4. **Pruning (for Decision Trees):**\n",
    "   - For decision tree-based models, pruning involves trimming the tree after it has been built. This prevents the tree from becoming too deep and capturing noise in the data.\n",
    "\n",
    "5. **Ensemble Methods:**\n",
    "   - Use ensemble methods like Random Forest or Gradient Boosting, which combine multiple models to reduce overfitting. Ensemble methods are robust and can compensate for weaknesses in individual models.\n",
    "\n",
    "6. **Reduce Model Complexity:**\n",
    "   - Choose simpler models or reduce the complexity of existing models. This may involve decreasing the number of layers in a neural network or reducing the polynomial degree in a polynomial regression.\n",
    "\n",
    "7. **Data Augmentation:**\n",
    "   - Increase the diversity of the training data by applying techniques such as data augmentation. This helps expose the model to a broader range of scenarios, making it more robust.\n",
    "\n",
    "8. **Dropout (for Neural Networks):**\n",
    "   - Introduce dropout layers in neural networks during training. Dropout randomly deactivates a fraction of neurons, preventing the model from relying too heavily on specific neurons and promoting generalization.\n",
    "\n",
    "9. **Early Stopping:**\n",
    "   - Monitor the model's performance on a validation set during training. Stop training when the performance on the validation set starts to degrade, preventing the model from overfitting the training data.\n",
    "\n",
    "10. **Hyperparameter Tuning:**\n",
    "    - Fine-tune hyperparameters, such as learning rates or regularization strengths, to find the right balance between model complexity and generalization.\n",
    "\n",
    "By implementing these techniques, machine learning practitioners can effectively reduce overfitting and build models that perform well on both the training and test datasets. The goal is to strike a balance between model complexity and the ability to generalize to new instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f045d07-dcb0-478b-a000-777bf0a90920",
   "metadata": {},
   "source": [
    "Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17961fc8-a6df-4250-8fc2-bb2161f9cf0b",
   "metadata": {},
   "source": [
    "**Underfitting:**\n",
    "\n",
    "Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the training data. The model fails to learn the relationships between the features and the target variable, leading to poor performance on both the training and test datasets.\n",
    "\n",
    "**Scenarios where Underfitting can Occur:**\n",
    "\n",
    "1. **Insufficient Model Complexity:**\n",
    "   - **Scenario:** Using a linear regression model to predict a highly nonlinear relationship in the data.\n",
    "   - **Explanation:** Linear models may be too simplistic to capture complex patterns, resulting in underfitting.\n",
    "\n",
    "2. **Few Features or Irrelevant Features:**\n",
    "   - **Scenario:** Training a model with too few features or features that do not adequately represent the relationships in the data.\n",
    "   - **Explanation:** Lack of informative features may lead to a model that cannot capture the complexity of the underlying patterns.\n",
    "\n",
    "3. **Low Polynomial Degree (for Polynomial Regression):**\n",
    "   - **Scenario:** Fitting a low-degree polynomial regression to data with a higher-degree underlying function.\n",
    "   - **Explanation:** Low-degree polynomials lack the flexibility to model intricate relationships, leading to underfitting.\n",
    "\n",
    "4. **Small Training Dataset:**\n",
    "   - **Scenario:** Training a complex model with a small amount of data.\n",
    "   - **Explanation:** Limited data may not provide sufficient information for the model to learn the underlying patterns, resulting in underfitting.\n",
    "\n",
    "5. **Ignoring Interaction Terms:**\n",
    "   - **Scenario:** Omitting interaction terms in a model when the relationship between variables involves interactions.\n",
    "   - **Explanation:** Ignoring interactions may lead to a model that fails to capture important relationships, resulting in underfitting.\n",
    "\n",
    "6. **Overly Regularized Model:**\n",
    "   - **Scenario:** Applying strong regularization (e.g., too high a penalty term in L1 or L2 regularization).\n",
    "   - **Explanation:** Excessive regularization discourages the model from learning complex patterns, leading to underfitting.\n",
    "\n",
    "7. **Ignoring Domain Knowledge:**\n",
    "   - **Scenario:** Neglecting domain-specific knowledge when selecting or configuring a model.\n",
    "   - **Explanation:** Failing to incorporate relevant domain knowledge may result in choosing a model that is too simplistic for the problem at hand.\n",
    "\n",
    "8. **Using a Simple Algorithm for a Complex Task:**\n",
    "   - **Scenario:** Using a basic algorithm (e.g., a single decision tree) for a task that requires a more sophisticated approach.\n",
    "   - **Explanation:** Simple algorithms may lack the capacity to model complex relationships, resulting in underfitting.\n",
    "\n",
    "Addressing underfitting involves increasing model complexity, introducing more relevant features, and choosing appropriate algorithms for the task at hand. It's essential to strike a balance to ensure the model is complex enough to capture patterns without being overly complex and prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e500488-73f6-45ed-b9c6-235a6829897e",
   "metadata": {},
   "source": [
    "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08895c-5143-4a67-a61e-58e1b201e61a",
   "metadata": {},
   "source": [
    "**Bias-Variance Tradeoff:**\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that involves finding the right balance between model complexity and the ability to generalize to new, unseen data. It describes the tradeoff between two sources of error that affect a model's performance: bias and variance.\n",
    "\n",
    "1. **Bias:**\n",
    "   - **Definition:** Bias is the error introduced by approximating a real-world problem with a simplified model. It represents the model's tendency to consistently underpredict or overpredict the true values.\n",
    "   - **Characteristics:**\n",
    "     - High bias models are too simplistic and often fail to capture complex patterns in the data.\n",
    "     - They may generalize poorly to both the training and test datasets.\n",
    "   - **Example:** A linear regression model applied to a dataset with a nonlinear relationship.\n",
    "\n",
    "2. **Variance:**\n",
    "   - **Definition:** Variance is the error introduced by the model's sensitivity to fluctuations in the training data. It measures the model's tendency to be highly responsive to changes in the training set.\n",
    "   - **Characteristics:**\n",
    "     - High variance models are overly complex and can capture noise in the training data.\n",
    "     - They may perform well on the training data but poorly on new, unseen data.\n",
    "   - **Example:** A high-degree polynomial regression model applied to a dataset with limited samples.\n",
    "\n",
    "**Relationship between Bias and Variance:**\n",
    "\n",
    "- **High Bias, Low Variance:**\n",
    "  - Simple models with high bias and low variance tend to oversimplify the underlying patterns in the data. They are consistent but may fail to capture complex relationships.\n",
    "\n",
    "- **Low Bias, High Variance:**\n",
    "  - Complex models with low bias and high variance can fit the training data very closely, including noise. However, they may fail to generalize well to new data.\n",
    "\n",
    "**Tradeoff:**\n",
    "\n",
    "- **Bias-Variance Tradeoff:**\n",
    "  - There is an inherent tradeoff between bias and variance. As model complexity increases, bias decreases but variance increases, and vice versa.\n",
    "  - Finding the right balance is crucial for building models that generalize well and perform effectively on new instances.\n",
    "\n",
    "**Implications for Model Performance:**\n",
    "\n",
    "- **Underfitting:**\n",
    "  - High bias, low variance models result in underfitting. They oversimplify the problem and fail to capture important patterns in the data.\n",
    "\n",
    "- **Overfitting:**\n",
    "  - Low bias, high variance models result in overfitting. They fit the training data too closely, capturing noise and failing to generalize.\n",
    "\n",
    "**Mitigating the Bias-Variance Tradeoff:**\n",
    "\n",
    "- **Regularization:**\n",
    "  - Use regularization techniques to penalize overly complex models and reduce variance.\n",
    "\n",
    "- **Feature Engineering:**\n",
    "  - Select relevant features and engineer informative features to reduce bias.\n",
    "\n",
    "- **Ensemble Methods:**\n",
    "  - Use ensemble methods like Random Forest or Gradient Boosting to combine multiple models and balance bias and variance.\n",
    "\n",
    "- **Cross-Validation:**\n",
    "  - Employ cross-validation techniques to assess model performance on different subsets of the training data.\n",
    "\n",
    "Achieving an optimal bias-variance tradeoff is essential for building models that generalize well, perform effectively on new data, and avoid the pitfalls of underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba42a501-3bb1-4c01-bd3d-ed411890ed65",
   "metadata": {},
   "source": [
    "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc0ea8b-0a16-4a92-8cb1-f4337691ebbe",
   "metadata": {},
   "source": [
    "Detecting overfitting and underfitting in machine learning models is crucial for building models that generalize well to new data. Here are some common methods to identify these issues:\n",
    "\n",
    "**1. Visual Inspection of Learning Curves:**\n",
    "   - **Overfitting:**\n",
    "     - Learning curves that show a significant gap between training and validation performance may indicate overfitting. The model is fitting the training data too closely but does not generalize well.\n",
    "   - **Underfitting:**\n",
    "     - Learning curves that show stagnation in both training and validation performance may indicate underfitting. The model is not learning the patterns in the data effectively.\n",
    "\n",
    "**2. Cross-Validation:**\n",
    "   - **Overfitting:**\n",
    "     - If a model performs exceptionally well on the training set but poorly on validation or test sets, it may be overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Consistently poor performance across training, validation, and test sets may indicate underfitting.\n",
    "\n",
    "**3. Model Evaluation Metrics:**\n",
    "   - **Overfitting:**\n",
    "     - A model that has significantly better performance on the training set compared to the validation or test set may be overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Consistently poor performance across different datasets may indicate underfitting.\n",
    "\n",
    "**4. Learning Curve Analysis:**\n",
    "   - **Overfitting:**\n",
    "     - A learning curve with decreasing training error but increasing validation error may indicate overfitting.\n",
    "   - **Underfitting:**\n",
    "     - A learning curve with slow convergence and high training and validation errors may indicate underfitting.\n",
    "\n",
    "**5. Feature Importance Analysis:**\n",
    "   - **Overfitting:**\n",
    "     - If a model assigns high importance to features that are noise or irrelevant, it may be overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Low feature importance across the board may indicate underfitting.\n",
    "\n",
    "**6. Regularization Parameter Tuning:**\n",
    "   - **Overfitting:**\n",
    "     - Increasing the regularization strength may help reduce overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Decreasing the regularization strength may help alleviate underfitting.\n",
    "\n",
    "**7. Residual Analysis (Regression Problems):**\n",
    "   - **Overfitting:**\n",
    "     - If residuals show a pattern, especially with high deviations, the model may be overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Large residuals or lack of fit in the residuals may indicate underfitting.\n",
    "\n",
    "**8. Ensemble Methods:**\n",
    "   - **Overfitting:**\n",
    "     - If a complex model overfits the training data, an ensemble method may help improve generalization.\n",
    "   - **Underfitting:**\n",
    "     - Ensemble methods may also help improve performance when individual models are underfitting.\n",
    "\n",
    "**9. Domain Knowledge:**\n",
    "   - **Overfitting:**\n",
    "     - If a model predicts values that are unlikely or contradict domain knowledge, it may be overfitting.\n",
    "   - **Underfitting:**\n",
    "     - Inconsistencies with known patterns or relationships in the data may indicate underfitting.\n",
    "\n",
    "By employing these methods, practitioners can gain insights into whether their models are overfitting, underfitting, or achieving an appropriate bias-variance tradeoff. Adjustments to model complexity, feature selection, or regularization can then be made accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e54b2d-3d39-467a-9bb7-5dd85bd623c6",
   "metadata": {},
   "source": [
    "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12138285-8b67-4ab0-9da4-cb198e7ab9ab",
   "metadata": {},
   "source": [
    "**Bias and Variance in Machine Learning:**\n",
    "\n",
    "**Bias:**\n",
    "- **Definition:** Bias refers to the error introduced by approximating a real-world problem with a simplified model. It represents the model's tendency to consistently underpredict or overpredict the true values.\n",
    "- **Characteristics:**\n",
    "  - High bias models are too simplistic and often fail to capture complex patterns in the data.\n",
    "  - They are associated with underfitting, where the model is unable to learn the underlying relationships.\n",
    "- **Example:** A linear regression model applied to a dataset with a nonlinear relationship.\n",
    "\n",
    "**Variance:**\n",
    "- **Definition:** Variance is the error introduced by the model's sensitivity to fluctuations in the training data. It measures the model's tendency to be highly responsive to changes in the training set.\n",
    "- **Characteristics:**\n",
    "  - High variance models are overly complex and can capture noise in the training data.\n",
    "  - They are associated with overfitting, where the model fits the training data too closely but generalizes poorly to new, unseen data.\n",
    "- **Example:** A high-degree polynomial regression model applied to a dataset with limited samples.\n",
    "\n",
    "**Comparison:**\n",
    "\n",
    "1. **Performance on Training Data:**\n",
    "   - **High Bias:**\n",
    "     - Performs poorly on the training data.\n",
    "     - Unable to capture complex patterns.\n",
    "   - **High Variance:**\n",
    "     - Performs well on the training data.\n",
    "     - Captures noise and fluctuations.\n",
    "\n",
    "2. **Performance on Test Data:**\n",
    "   - **High Bias:**\n",
    "     - Performs poorly on test data (generalization error is high).\n",
    "   - **High Variance:**\n",
    "     - Performs poorly on test data (generalization error is high).\n",
    "\n",
    "3. **Underlying Issue:**\n",
    "   - **High Bias:**\n",
    "     - The model is too simplistic.\n",
    "     - Fails to learn the underlying patterns.\n",
    "   - **High Variance:**\n",
    "     - The model is overly complex.\n",
    "     - Fits the training data too closely, including noise.\n",
    "\n",
    "4. **Remedies:**\n",
    "   - **High Bias:**\n",
    "     - Increase model complexity.\n",
    "     - Add more features.\n",
    "   - **High Variance:**\n",
    "     - Decrease model complexity.\n",
    "     - Regularize the model.\n",
    "     - Increase the size of the training dataset.\n",
    "\n",
    "5. **Tradeoff:**\n",
    "   - **Bias-Variance Tradeoff:**\n",
    "     - There is a tradeoff between bias and variance. As one decreases, the other increases, and vice versa.\n",
    "     - The goal is to find an optimal balance for the given problem.\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. **High Bias Model:**\n",
    "   - **Example:** Linear regression on a dataset with a complex, nonlinear relationship.\n",
    "   - **Performance:**\n",
    "     - Fails to capture the underlying patterns.\n",
    "     - Both training and test errors are high.\n",
    "\n",
    "2. **High Variance Model:**\n",
    "   - **Example:** A high-degree polynomial regression on a small dataset.\n",
    "   - **Performance:**\n",
    "     - Fits the training data closely, including noise.\n",
    "     - High training accuracy but poor generalization to new data.\n",
    "\n",
    "Understanding and managing the bias-variance tradeoff is essential for building models that generalize well to new, unseen data. Balancing bias and variance leads to models that achieve better performance on a variety of datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2b76f-4597-40f6-948d-c4025bbec4c4",
   "metadata": {},
   "source": [
    "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5586e5-ac2f-411c-b616-8e0e265902cc",
   "metadata": {},
   "source": [
    "**Regularization in Machine Learning:**\n",
    "\n",
    "**Definition:**\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the objective function, discouraging overly complex models. The goal is to find a balance between fitting the training data well and avoiding excessive complexity that may hinder generalization to new, unseen data.\n",
    "\n",
    "**Objective Function with Regularization Term:**\n",
    "The regularized objective function is often a combination of the original objective function (e.g., mean squared error for regression or cross-entropy for classification) and a regularization term. The regularization term penalizes large weights or high model complexity.\n",
    "\n",
    "**Common Regularization Techniques:**\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - **Penalty Term:** λ * Σ|w_i|\n",
    "   - **Effect:**\n",
    "     - Encourages sparsity by setting some weights to exactly zero.\n",
    "     - Can be useful for feature selection.\n",
    "   - **Use Case:**\n",
    "     - When there is a suspicion that some features are irrelevant.\n",
    "\n",
    "2. **L2 Regularization (Ridge):**\n",
    "   - **Penalty Term:** λ * Σw_i^2\n",
    "   - **Effect:**\n",
    "     - Discourages overly large weights but doesn't set them exactly to zero.\n",
    "     - Distributes the penalty across all weights.\n",
    "   - **Use Case:**\n",
    "     - Generally applied when all features are expected to contribute.\n",
    "\n",
    "3. **Elastic Net:**\n",
    "   - **Penalty Term:** α * L1 + (1 - α) * L2\n",
    "   - **Effect:**\n",
    "     - Combines L1 and L2 regularization.\n",
    "     - Allows tuning the tradeoff between sparsity and smoothness.\n",
    "   - **Use Case:**\n",
    "     - A hybrid approach when both feature selection and regularization are desired.\n",
    "\n",
    "4. **Dropout (Neural Networks):**\n",
    "   - **Effect:**\n",
    "     - Randomly drops a fraction of neurons during training, preventing reliance on specific neurons and encouraging robustness.\n",
    "   - **Use Case:**\n",
    "     - Particularly effective in neural networks.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **Effect:**\n",
    "     - Halts the training process when the model performance on a validation set starts to degrade.\n",
    "     - Prevents the model from learning noise in the training data.\n",
    "   - **Use Case:**\n",
    "     - Commonly used in iterative optimization algorithms.\n",
    "\n",
    "6. **Batch Normalization:**\n",
    "   - **Effect:**\n",
    "     - Normalizes the input of each layer to have zero mean and unit variance.\n",
    "     - Can act as a form of regularization by reducing internal covariate shift.\n",
    "   - **Use Case:**\n",
    "     - Improves training stability in deep neural networks.\n",
    "\n",
    "7. **Weight Decay:**\n",
    "   - **Penalty Term:** λ * Σw_i^2\n",
    "   - **Effect:**\n",
    "     - Similar to L2 regularization, penalizing large weights.\n",
    "   - **Use Case:**\n",
    "     - Commonly used in linear models and neural networks.\n",
    "\n",
    "**How Regularization Prevents Overfitting:**\n",
    "Regularization penalizes overly complex models by adding a penalty term to the objective function, which discourages extreme parameter values. This prevents the model from fitting the training data too closely and helps generalize better to new, unseen data. By controlling the complexity of the model, regularization aids in finding a balance between bias and variance, mitigating the risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f379f-1613-446c-a1cc-8278db833e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
