{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0efb21c9-8d57-48cc-885a-d784f6fb0bf9",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c191bb-3854-494f-bbb8-5681aa24f956",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning involves combining the predictions of multiple base models to create a stronger, more robust model. The idea behind ensemble methods is to leverage the diversity of individual models to improve overall performance, generalization, and robustness. Ensemble techniques are widely used in various machine learning tasks and are known for their ability to achieve higher accuracy than individual models.\n",
    "\n",
    "There are several types of ensemble techniques, and the two main categories are:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):**\n",
    "   - In bagging, multiple instances of the same base model are trained on different subsets of the training data, often created through bootstrapping (sampling with replacement).\n",
    "   - The predictions of individual models are then combined, typically by averaging (for regression) or voting (for classification).\n",
    "   - Random Forest is a popular bagging ensemble method that uses decision trees as base models.\n",
    "\n",
    "2. **Boosting:**\n",
    "   - In boosting, multiple weak learners (models that perform slightly better than random chance) are trained sequentially, with each model focusing on the mistakes of its predecessor.\n",
    "   - The final prediction is a weighted combination of the predictions of all models, with more weight given to models that perform well.\n",
    "   - AdaBoost, Gradient Boosting, and XGBoost are popular boosting algorithms.\n",
    "\n",
    "Ensemble techniques offer several advantages:\n",
    "\n",
    "- **Improved Performance:** Ensembles can achieve higher accuracy than individual models, especially when combining diverse models.\n",
    "- **Reduced Overfitting:** By combining models with different sources of error, ensembles can often reduce overfitting and generalize better to new, unseen data.\n",
    "- **Robustness:** Ensembles are less sensitive to noise and outliers in the data, making them more robust in various scenarios.\n",
    "- **Versatility:** Ensembles can be applied to various types of models, making them versatile in addressing different types of machine learning problems.\n",
    "\n",
    "Ensemble techniques play a crucial role in machine learning, and their effectiveness has been demonstrated across a wide range of applications, including classification, regression, and anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ab930-5cd2-41d1-bca3-1b10c9b54fb4",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8852aa59-eb85-47b6-aa53-a6f0294b46c7",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several reasons, and they provide various advantages that contribute to improved model performance, generalization, and robustness. Here are some key reasons why ensemble techniques are widely used:\n",
    "\n",
    "1. **Increased Accuracy:**\n",
    "   - Ensemble methods can often achieve higher accuracy compared to individual models. By combining the predictions of multiple models, ensemble techniques leverage the strengths of each model to compensate for their individual weaknesses.\n",
    "\n",
    "2. **Reduced Overfitting:**\n",
    "   - Overfitting occurs when a model learns the training data too well and performs poorly on new, unseen data. Ensemble methods, especially bagging techniques like Random Forest, can reduce overfitting by aggregating the predictions of multiple models trained on different subsets of the data.\n",
    "\n",
    "3. **Improved Generalization:**\n",
    "   - Ensemble methods enhance the generalization of models. By combining diverse models that capture different aspects of the underlying patterns in the data, ensembles can make more robust predictions on new, unseen data.\n",
    "\n",
    "4. **Robustness to Noise and Outliers:**\n",
    "   - Ensembles are less sensitive to noise and outliers in the data. Outliers or noisy instances that may strongly influence a single model are less likely to have a significant impact on the overall ensemble, resulting in more robust predictions.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques are algorithm-agnostic, meaning they can be applied to various types of base models. Whether using decision trees, neural networks, or other algorithms as base models, ensemble methods can combine their predictions to create a more powerful model.\n",
    "\n",
    "6. **Handling Model Uncertainty:**\n",
    "   - Ensembles provide a way to quantify model uncertainty. By aggregating predictions from multiple models, ensembles can provide not only a point estimate but also a measure of uncertainty or variability in predictions.\n",
    "\n",
    "7. **Effective in High-Dimensional Spaces:**\n",
    "   - In high-dimensional feature spaces, where the curse of dimensionality can affect model performance, ensembles can be particularly effective. They help capture complex interactions and patterns in the data.\n",
    "\n",
    "8. **Simple Models, Strong Predictions:**\n",
    "   - Ensembles allow the use of simple base models (weak learners) that individually may not perform well, but when combined, they contribute to strong overall predictions.\n",
    "\n",
    "9. **Flexibility for Different Tasks:**\n",
    "   - Ensemble techniques are versatile and can be applied to various machine learning tasks, including classification, regression, and anomaly detection.\n",
    "\n",
    "Overall, ensemble techniques provide a powerful and flexible approach to building robust and accurate machine learning models, making them a valuable tool in the data scientist's toolkit. Popular ensemble methods include Random Forest, AdaBoost, Gradient Boosting, and bagging techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392495e7-4614-4f60-b351-e547d37d9a13",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1409d0-7afa-43db-a712-9528e2111777",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that involves training multiple instances of the same base model on different subsets of the training data. The main idea behind bagging is to reduce variance and improve the stability and accuracy of the model by leveraging the diversity of multiple models.\n",
    "\n",
    "The key steps in the bagging process are as follows:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Randomly select subsets of the training data with replacement (bootstrap samples). Each subset is of the same size as the original training dataset.\n",
    "   - Some instances may appear multiple times in a subset, while others may be omitted.\n",
    "\n",
    "2. **Model Training:**\n",
    "   - Train a separate instance of the base model (classifier or regressor) on each bootstrap sample. This results in multiple base models, each trained on a slightly different subset of the data.\n",
    "\n",
    "3. **Predictions:**\n",
    "   - For classification tasks, when making predictions, the final output is often determined by a majority vote (voting). For regression tasks, the predictions are typically averaged.\n",
    "\n",
    "Bagging has several advantages:\n",
    "\n",
    "- **Reduced Variance:** By training models on different subsets of the data, bagging reduces the variance of the model, making it less sensitive to the specific instances in the training set.\n",
    "  \n",
    "- **Improved Accuracy:** Bagging often leads to more accurate predictions compared to individual models, especially when the base model tends to overfit.\n",
    "\n",
    "- **Robustness:** Bagging is less prone to outliers and noise in the data because the impact of individual instances is diminished when averaging or voting is applied.\n",
    "\n",
    "- **Parallelization:** The training of each base model can be done independently, allowing for parallelization and faster training.\n",
    "\n",
    "One of the most popular bagging algorithms is the **Random Forest**, which uses bagging as its underlying mechanism but introduces additional randomness by considering only a random subset of features at each split in a decision tree.\n",
    "\n",
    "Bagging can be applied to various types of base models, including decision trees, support vector machines, and other machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473cd9a7-8c25-4a9e-85fe-9d0d3f960510",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff53729-997b-447d-987d-9764fe076366",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning technique in machine learning that combines the predictions of multiple weak learners (models that perform slightly better than random chance) to create a strong learner. The primary idea behind boosting is to sequentially train weak models, giving more emphasis to instances that were misclassified by the previous models. The final prediction is a weighted sum of the predictions of all weak learners.\n",
    "\n",
    "The key steps in the boosting process are as follows:\n",
    "\n",
    "1. **Sequential Training:**\n",
    "   - Train a sequence of weak learners (usually decision trees or shallow models) sequentially.\n",
    "   - Each model is trained to correct the mistakes made by the previous models, with a focus on the instances that were misclassified.\n",
    "\n",
    "2. **Weighted Voting or Averaging:**\n",
    "   - Assign weights to each weak learner based on its performance. Models that perform well are given higher weights in the final prediction.\n",
    "   - The final prediction is typically a weighted sum of the predictions of all weak learners.\n",
    "\n",
    "3. **Adaptive Learning:**\n",
    "   - Adjust the weights of misclassified instances at each iteration to guide the learning process. This allows boosting to focus on difficult-to-classify instances.\n",
    "\n",
    "4. **Stopping Criterion:**\n",
    "   - The boosting process continues until a predetermined number of weak learners are trained or until a stopping criterion is met (e.g., perfect classification on the training set).\n",
    "\n",
    "Boosting has several advantages:\n",
    "\n",
    "- **Improved Accuracy:** Boosting often results in higher accuracy compared to individual models, especially when dealing with complex and challenging datasets.\n",
    "  \n",
    "- **Handling Weak Models:** Boosting can effectively combine weak models to create a strong and accurate predictive model.\n",
    "  \n",
    "- **Adaptability:** Boosting adapts to the complexity of the dataset by assigning more importance to instances that are difficult to classify.\n",
    "\n",
    "Popular boosting algorithms include:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):**\n",
    "   - Assigns weights to instances and focuses on misclassified instances in subsequent iterations.\n",
    "\n",
    "2. **Gradient Boosting:**\n",
    "   - Builds a sequence of trees where each tree corrects the errors of the previous one. Popular implementations include XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "3. **Stochastic Gradient Boosting:**\n",
    "   - Similar to gradient boosting but introduces stochasticity by using random subsets of data for training each tree.\n",
    "\n",
    "Boosting is a powerful technique but may be sensitive to noise and outliers. Care should be taken to tune hyperparameters, and regularization techniques are sometimes applied to prevent overfitting. Overall, boosting is widely used in practice and has proven effective in a variety of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d639df87-669d-4d04-9a5b-9f905de5a4c0",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83030c-aeb1-497a-bb01-df89f543e672",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several benefits in machine learning, contributing to improved model performance, generalization, and robustness. Here are some key benefits of using ensemble techniques:\n",
    "\n",
    "1. **Increased Accuracy:**\n",
    "   - Ensemble methods often lead to higher accuracy compared to individual models. By combining the predictions of multiple models, ensembles leverage the strengths of each model, compensating for their individual weaknesses.\n",
    "\n",
    "2. **Reduced Overfitting:**\n",
    "   - Overfitting occurs when a model learns the training data too well and performs poorly on new, unseen data. Ensemble methods, especially bagging techniques, can reduce overfitting by combining predictions from models trained on different subsets of the data.\n",
    "\n",
    "3. **Improved Generalization:**\n",
    "   - Ensembles enhance the generalization of models. By combining diverse models that capture different aspects of the underlying patterns in the data, ensembles can make more robust predictions on new, unseen data.\n",
    "\n",
    "4. **Robustness to Noise and Outliers:**\n",
    "   - Ensembles are less sensitive to noise and outliers in the data. Outliers or noisy instances that may strongly influence a single model are less likely to have a significant impact on the overall ensemble, resulting in more robust predictions.\n",
    "\n",
    "5. **Versatility Across Algorithms:**\n",
    "   - Ensemble techniques are algorithm-agnostic, meaning they can be applied to various types of base models. Whether using decision trees, neural networks, or other algorithms as base models, ensemble methods can combine their predictions to create a more powerful model.\n",
    "\n",
    "6. **Effective in High-Dimensional Spaces:**\n",
    "   - In high-dimensional feature spaces, where the curse of dimensionality can affect model performance, ensembles can be particularly effective. They help capture complex interactions and patterns in the data.\n",
    "\n",
    "7. **Handling Model Uncertainty:**\n",
    "   - Ensembles provide a way to quantify model uncertainty. By aggregating predictions from multiple models, ensembles can provide not only a point estimate but also a measure of uncertainty or variability in predictions.\n",
    "\n",
    "8. **Simple Models, Strong Predictions:**\n",
    "   - Ensembles allow the use of simple base models (weak learners) that individually may not perform well, but when combined, they contribute to strong overall predictions.\n",
    "\n",
    "9. **Flexibility for Different Tasks:**\n",
    "   - Ensemble techniques are versatile and can be applied to various machine learning tasks, including classification, regression, and anomaly detection.\n",
    "\n",
    "10. **Parallelization:**\n",
    "    - The training of each base model in ensemble methods can often be done independently, allowing for parallelization and faster training.\n",
    "\n",
    "Ensemble techniques play a crucial role in machine learning, and their effectiveness has been demonstrated across a wide range of applications. They are considered a powerful tool for improving model performance, especially in complex and challenging scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbe189-96a5-4de6-9330-63b24c89d4ca",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401e382c-badb-4e6d-bda7-6014efebbf38",
   "metadata": {},
   "source": [
    "While ensemble techniques often outperform individual models in terms of accuracy and robustness, there are situations where ensemble methods may not necessarily be superior. Here are some considerations:\n",
    "\n",
    "1. **Data Quality:**\n",
    "   - If the dataset is small or of low quality, individual models may struggle to provide meaningful predictions. In such cases, ensembles might not be significantly better than individual models.\n",
    "\n",
    "2. **Computational Resources:**\n",
    "   - Ensembles, especially those with a large number of models or complex algorithms, can be computationally expensive. In situations where computational resources are limited, using a single, well-tuned model may be more practical.\n",
    "\n",
    "3. **Interpretability:**\n",
    "   - Ensemble models are often seen as \"black boxes\" due to their complexity, making them less interpretable compared to simpler models. In scenarios where interpretability is crucial, an individual model may be preferred.\n",
    "\n",
    "4. **Data Characteristics:**\n",
    "   - If the dataset is inherently simple and exhibits clear patterns, a single well-tuned model might already capture the relationships in the data, and the additional complexity of an ensemble may not be necessary.\n",
    "\n",
    "5. **Overfitting in Ensemble Models:**\n",
    "   - While ensembles can reduce overfitting, there's a risk of overfitting to the training data if the ensemble becomes too complex or if not enough regularization is applied. In such cases, the ensemble may not generalize well to new data.\n",
    "\n",
    "6. **Noisy or Misleading Models:**\n",
    "   - If some of the individual models in the ensemble are noisy or provide misleading predictions, they can negatively impact the overall performance of the ensemble.\n",
    "\n",
    "7. **Difficulty in Model Combination:**\n",
    "   - Integrating the predictions of individual models in a way that enhances overall performance can be challenging. In some cases, simple averaging or voting may not be sufficient, and more sophisticated techniques may be required.\n",
    "\n",
    "8. **Algorithm Choice:**\n",
    "   - The choice of ensemble algorithm matters. Different ensemble methods have different strengths and weaknesses. For example, if the base models are highly correlated, the benefits of diversity are diminished.\n",
    "\n",
    "9. **Tuning Complexity:**\n",
    "   - Ensembles may require more hyperparameter tuning compared to individual models. Tuning the ensemble's parameters can be a complex process and may require additional effort.\n",
    "\n",
    "In practice, the decision to use an ensemble or an individual model depends on the specific characteristics of the data, the problem at hand, available computational resources, and the trade-off between interpretability and predictive performance. It's often advisable to experiment with both approaches and evaluate their performance on relevant metrics before making a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ce5ab6-7566-48b1-94b7-867c6a54c8db",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546dc3d-6bfc-4f10-8970-53fde7df4919",
   "metadata": {},
   "source": [
    "The bootstrap method is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from the observed data. The confidence interval using the bootstrap method involves generating a large number of bootstrap samples and then calculating the desired statistic (e.g., mean, median, standard deviation) for each sample. The confidence interval is then constructed based on the distribution of these bootstrap statistics.\n",
    "\n",
    "Here's a general outline of the steps to calculate a confidence interval using bootstrap:\n",
    "\n",
    "1. **Collect the Original Data:**\n",
    "   - Begin with the original dataset containing observed values.\n",
    "\n",
    "2. **Generate Bootstrap Samples:**\n",
    "   - Randomly draw samples (with replacement) from the original dataset to create multiple bootstrap samples. Each bootstrap sample has the same size as the original dataset.\n",
    "\n",
    "3. **Calculate the Statistic:**\n",
    "   - For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation).\n",
    "\n",
    "4. **Repeat the Process:**\n",
    "   - Repeat steps 2 and 3 a large number of times (e.g., 1,000 or 10,000 times) to obtain a distribution of the statistic.\n",
    "\n",
    "5. **Construct the Confidence Interval:**\n",
    "   - Based on the distribution of bootstrap statistics, determine the confidence interval. The interval is typically constructed by finding the percentile values that correspond to the desired level of confidence.\n",
    "     - For a 95% confidence interval, you might use the 2.5th percentile as the lower bound and the 97.5th percentile as the upper bound.\n",
    "\n",
    "The formula for calculating a confidence interval using the bootstrap method is as follows:\n",
    "\n",
    "\\[ \\text{Confidence Interval} = [\\text{Percentile}_{\\alpha/2}, \\text{Percentile}_{1-\\alpha/2}] \\]\n",
    "\n",
    "where \\(\\alpha\\) is the significance level (e.g., 0.05 for a 95% confidence interval), and \\(\\text{Percentile}_{\\alpha/2}\\) and \\(\\text{Percentile}_{1-\\alpha/2}\\) are the percentiles corresponding to \\(\\alpha/2\\) and \\(1-\\alpha/2\\) in the bootstrap distribution.\n",
    "\n",
    "It's important to note that the bootstrap method assumes that the observed data is representative of the population, and the resampling is done with replacement to mimic the randomness of the sampling process. The confidence interval obtained through bootstrapping provides an estimate of the uncertainty associated with the statistic of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840f4c80-070c-4ea4-9aca-146e2bfd5485",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902ca2cb-7ec8-47f3-9625-90f77634e888",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics to estimate the sampling distribution of a statistic by repeatedly resampling from the observed data. The method involves creating multiple bootstrap samples by drawing observations from the original dataset with replacement. The primary goal is to assess the variability and uncertainty of a statistic without making strong parametric assumptions.\n",
    "\n",
    "Here are the steps involved in the bootstrap procedure:\n",
    "\n",
    "1. **Collect the Original Data:**\n",
    "   - Start with the original dataset containing observed values.\n",
    "\n",
    "2. **Random Sampling with Replacement:**\n",
    "   - Randomly draw \\(n\\) observations (where \\(n\\) is the size of the original dataset) from the original dataset with replacement to form a bootstrap sample.\n",
    "   - Some observations may appear multiple times, while others may be omitted.\n",
    "\n",
    "3. **Calculate the Statistic:**\n",
    "   - Compute the desired statistic of interest (e.g., mean, median, standard deviation) for the current bootstrap sample.\n",
    "\n",
    "4. **Repeat Steps 2 and 3:**\n",
    "   - Repeat the random sampling and statistic calculation process a large number of times (typically thousands or tens of thousands) to create multiple bootstrap samples and their corresponding statistics.\n",
    "\n",
    "5. **Construct the Bootstrap Distribution:**\n",
    "   - Collect the computed statistics from all bootstrap samples to create the bootstrap distribution of the statistic.\n",
    "\n",
    "6. **Estimate Confidence Intervals:**\n",
    "   - Use the bootstrap distribution to estimate confidence intervals for the statistic. Commonly, percentiles (e.g., 2.5th and 97.5th percentiles for a 95% confidence interval) are used to form the interval.\n",
    "\n",
    "The key idea behind bootstrap is to simulate the process of drawing multiple samples from the population by repeatedly resampling from the observed data. This allows practitioners to estimate the uncertainty associated with a statistic without making strong assumptions about the underlying population distribution.\n",
    "\n",
    "Bootstrap is widely used in various statistical analyses, such as estimating confidence intervals, assessing bias and variance, and constructing hypothesis tests. It provides a powerful and flexible tool for situations where parametric methods may be challenging due to the lack of distributional assumptions or complex dependencies within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6093b-9e54-4aff-a3d7-4b10897fd70c",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985b752-1da6-4673-965e-4a8510340e00",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we'll follow the steps outlined earlier. The procedure involves resampling with replacement from the observed sample and calculating the mean for each bootstrap sample. We then use the distribution of bootstrap means to construct the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6f3867-dc53-482b-93dd-cb41cd50284c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean Height: [14.37501159 15.55385223]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given sample data\n",
    "sample_heights = np.random.normal(loc=15, scale=2, size=50)  # Simulating sample data\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_bootstrap_samples)\n",
    "\n",
    "# Bootstrap procedure\n",
    "for i in range(num_bootstrap_samples):\n",
    "    # Resample with replacement\n",
    "    bootstrap_sample = np.random.choice(sample_heights, size=len(sample_heights), replace=True)\n",
    "    \n",
    "    # Calculate mean for the bootstrap sample\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Print the results\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c503fb6-1056-4edf-80b5-1bacc48600b2",
   "metadata": {},
   "source": [
    "In this example, I've used the NumPy library to simulate a sample of tree heights with a mean of 15 meters and a standard deviation of 2 meters. The bootstrap procedure involves resampling from this sample, calculating the mean for each bootstrap sample, and then estimating the 95% confidence interval based on the distribution of bootstrap means.\n",
    "\n",
    "Note: In a real-world scenario, you would replace the simulated data with the actual measured heights of the 50 trees. The code serves as an illustrative example of the bootstrap procedure for estimating a confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d1db7-70b1-4631-985a-e934dc2741ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
