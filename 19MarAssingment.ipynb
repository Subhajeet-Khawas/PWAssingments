{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "479f52c6-3744-4bc6-b76f-a7d89df694ec",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeef3ab-d196-4ad5-8d8f-558674bd7d0a",
   "metadata": {},
   "source": [
    "**Min-Max Scaling:**\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to scale and transform the features of a dataset to a specific range. The goal is to ensure that all features have the same scale, preventing certain features from dominating others in cases where they have different magnitudes.\n",
    "\n",
    "The formula for Min-Max scaling is given by:\n",
    "\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where:\n",
    "- \\(X\\) is the original feature value.\n",
    "- \\(X_{\\text{min}}\\) is the minimum value of the feature in the dataset.\n",
    "- \\(X_{\\text{max}}\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "The result is that the scaled feature values fall within the range [0, 1].\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with a feature, \"Income,\" where the original income values range from $30,000 to $90,000. The goal is to apply Min-Max scaling to bring these values into the range [0, 1].\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Income: [30000, 45000, 60000, 75000, 90000]\n",
    "\n",
    "2. **Min-Max Scaling:**\n",
    "   - Calculate \\(X_{\\text{min}}\\) and \\(X_{\\text{max}}\\) for the \"Income\" feature.\n",
    "     - \\(X_{\\text{min}} = 30000\\)\n",
    "     - \\(X_{\\text{max}} = 90000\\)\n",
    "\n",
    "   - Apply Min-Max scaling for each income value:\n",
    "     - \\(X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\\)\n",
    "\n",
    "     - \\(X_{\\text{scaled}} = \\frac{30000 - 30000}{90000 - 30000} = 0\\)\n",
    "     - \\(X_{\\text{scaled}} = \\frac{45000 - 30000}{90000 - 30000} = 0.25\\)\n",
    "     - \\(X_{\\text{scaled}} = \\frac{60000 - 30000}{90000 - 30000} = 0.5\\)\n",
    "     - \\(X_{\\text{scaled}} = \\frac{75000 - 30000}{90000 - 30000} = 0.75\\)\n",
    "     - \\(X_{\\text{scaled}} = \\frac{90000 - 30000}{90000 - 30000} = 1\\)\n",
    "\n",
    "3. **Scaled Data:**\n",
    "   - Scaled Income: [0, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "The Min-Max scaling ensures that the \"Income\" values are transformed to a common scale, making it easier for machine learning algorithms to work with the data, particularly when features have different units or ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714591a-81e8-4e5c-ab71-2004936d518d",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c521156-6d7d-4593-8299-615734eec0a1",
   "metadata": {},
   "source": [
    "**Unit Vector Technique in Feature Scaling:**\n",
    "\n",
    "The Unit Vector technique, also known as Unit Vector Scaling or Vector Normalization, is a feature scaling technique that scales the values of a feature to form a unit vector. A unit vector is a vector with a magnitude of 1. In this technique, each data point in the feature space is scaled by dividing it by the Euclidean norm (L2 norm) of the feature vector.\n",
    "\n",
    "The formula for Unit Vector Scaling is given by:\n",
    "\n",
    "\\[ \\text{Unit Vector} = \\frac{\\mathbf{X}}{\\|\\mathbf{X}\\|_2} \\]\n",
    "\n",
    "where:\n",
    "- \\(\\mathbf{X}\\) is the original feature vector.\n",
    "- \\(\\|\\mathbf{X}\\|_2\\) is the Euclidean norm of the feature vector.\n",
    "\n",
    "**Differences from Min-Max Scaling:**\n",
    "\n",
    "- **Range of Values:**\n",
    "  - Min-Max Scaling ensures that the scaled values fall within a specific range, often [0, 1].\n",
    "  - Unit Vector Scaling transforms the feature vector into a unit vector, and the magnitude of the unit vector is always 1.\n",
    "\n",
    "- **Direction Preservation:**\n",
    "  - Min-Max Scaling preserves the direction of the vector but scales its magnitude.\n",
    "  - Unit Vector Scaling preserves the direction of the vector and ensures that the vector becomes a unit vector.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with a feature, \"Height,\" where the original height values range from 150 cm to 180 cm. We want to apply the Unit Vector Scaling to transform the height values into a unit vector.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Height: [150, 160, 170, 175, 180]\n",
    "\n",
    "2. **Unit Vector Scaling:**\n",
    "   - Calculate the Euclidean norm (\\(\\|\\mathbf{X}\\|_2\\)) of the \"Height\" feature vector.\n",
    "\n",
    "   - Apply Unit Vector Scaling for each height value:\n",
    "     - \\(\\text{Unit Vector} = \\frac{\\mathbf{X}}{\\|\\mathbf{X}\\|_2}\\)\n",
    "\n",
    "     - \\(\\text{Unit Vector} = \\frac{150}{\\sqrt{150^2 + 160^2 + 170^2 + 175^2 + 180^2}}\\)\n",
    "     - \\(\\text{Unit Vector} = \\frac{160}{\\sqrt{150^2 + 160^2 + 170^2 + 175^2 + 180^2}}\\)\n",
    "     - \\(\\text{Unit Vector} = \\frac{170}{\\sqrt{150^2 + 160^2 + 170^2 + 175^2 + 180^2}}\\)\n",
    "     - \\(\\text{Unit Vector} = \\frac{175}{\\sqrt{150^2 + 160^2 + 170^2 + 175^2 + 180^2}}\\)\n",
    "     - \\(\\text{Unit Vector} = \\frac{180}{\\sqrt{150^2 + 160^2 + 170^2 + 175^2 + 180^2}}\\)\n",
    "\n",
    "3. **Scaled Data:**\n",
    "   - Scaled Height (Unit Vector): \\(\\left[\\frac{150}{\\|\\mathbf{X}\\|_2}, \\frac{160}{\\|\\mathbf{X}\\|_2}, \\frac{170}{\\|\\mathbf{X}\\|_2}, \\frac{175}{\\|\\mathbf{X}\\|_2}, \\frac{180}{\\|\\mathbf{X}\\|_2}\\right]\\)\n",
    "\n",
    "The Unit Vector Scaling ensures that the \"Height\" values are transformed into a unit vector, preserving the direction of the original feature vector while normalizing its magnitude to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351b190-33e3-4258-a2b3-a1aa7aaea47d",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2a41f7-0a80-42fa-ae43-facace826460",
   "metadata": {},
   "source": [
    "**PCA (Principal Component Analysis):**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in statistics and machine learning to transform high-dimensional data into a lower-dimensional representation. PCA identifies the principal components, which are linear combinations of the original features, such that they capture the maximum variance in the data. These principal components are orthogonal to each other, and the first few components typically retain most of the information in the original data.\n",
    "\n",
    "**Steps in PCA:**\n",
    "\n",
    "1. **Standardization:**\n",
    "   - Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2. **Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Sort Eigenvalues:**\n",
    "   - Sort the eigenvalues in descending order and choose the top \\(k\\) eigenvectors corresponding to the largest eigenvalues, where \\(k\\) is the desired dimensionality of the reduced data.\n",
    "\n",
    "5. **Projection:**\n",
    "   - Project the original data onto the selected eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with three features: \"Height,\" \"Weight,\" and \"Age.\" We want to apply PCA to reduce the dimensionality to two dimensions.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Height, Weight, Age for each data point.\n",
    "\n",
    "2. **Standardization:**\n",
    "   - Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "3. **Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "4. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\n",
    "5. **Sort Eigenvalues:**\n",
    "   - Sort the eigenvalues in descending order and choose the top two eigenvectors.\n",
    "\n",
    "6. **Projection:**\n",
    "   - Project the original data onto the selected eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "The result is a transformed dataset with two features (principal components) that capture the most significant variation in the original data.\n",
    "\n",
    "PCA is useful for reducing the dimensionality of data, removing redundant information, and often improving the performance of machine learning models by focusing on the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebece8-f609-44cb-95d4-7bcba6a17dd8",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001d7c24-3367-497d-8188-38f1fc014b72",
   "metadata": {},
   "source": [
    "**Relationship between PCA and Feature Extraction:**\n",
    "\n",
    "PCA (Principal Component Analysis) is a technique that can be used for feature extraction. Feature extraction is the process of transforming high-dimensional data into a lower-dimensional representation while retaining the most important information. PCA achieves feature extraction by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "**How PCA is Used for Feature Extraction:**\n",
    "\n",
    "1. **Standardization:**\n",
    "   - Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "2. **Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Sort Eigenvalues:**\n",
    "   - Sort the eigenvalues in descending order and choose the top \\(k\\) eigenvectors corresponding to the largest eigenvalues, where \\(k\\) is the desired dimensionality of the reduced data.\n",
    "\n",
    "5. **Projection:**\n",
    "   - Project the original data onto the selected eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a dataset with five features: \"F1,\" \"F2,\" \"F3,\" \"F4,\" and \"F5.\" We want to use PCA for feature extraction to reduce the dimensionality to three dimensions.\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Features: F1, F2, F3, F4, F5 for each data point.\n",
    "\n",
    "2. **Standardization:**\n",
    "   - Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "3. **Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "4. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\n",
    "5. **Sort Eigenvalues:**\n",
    "   - Sort the eigenvalues in descending order and choose the top three eigenvectors.\n",
    "\n",
    "6. **Projection:**\n",
    "   - Project the original data onto the selected three eigenvectors to obtain the reduced-dimensional representation.\n",
    "\n",
    "The resulting reduced-dimensional representation contains three features (principal components) that capture the most significant information from the original five features.\n",
    "\n",
    "In summary, PCA can be considered a form of feature extraction as it identifies and retains the most important features (principal components) that contribute to the variance in the data. This reduced set of features can be used in subsequent analysis or machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad1a1e-97c5-407f-ad16-52d2c8b51240",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d09a5-05d0-4465-826f-c05adde69d29",
   "metadata": {},
   "source": [
    "**Using Min-Max Scaling for Preprocessing in a Recommendation System:**\n",
    "\n",
    "Min-Max scaling is a feature scaling technique that transforms the values of a feature to a specific range, typically [0, 1]. This is achieved by subtracting the minimum value and dividing by the range (difference between maximum and minimum values). Min-Max scaling is useful when the features in the dataset have different scales, and it ensures that all features contribute equally to the analysis.\n",
    "\n",
    "In the context of building a recommendation system for a food delivery service with features like price, rating, and delivery time, here's how you would use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. **Understand the Features:**\n",
    "   - Identify the features in your dataset that require scaling. In this case, you have features such as price, rating, and delivery time.\n",
    "\n",
    "2. **Check Feature Distributions:**\n",
    "   - Examine the distributions of each feature to understand their ranges. Features like price and delivery time may have different scales.\n",
    "\n",
    "3. **Apply Min-Max Scaling:**\n",
    "   - For each feature that requires scaling, apply the Min-Max scaling transformation. The formula for Min-Max scaling is given by:\n",
    "     \\[ \\text{Scaled Value} = \\frac{\\text{Original Value} - \\text{Min Value}}{\\text{Max Value} - \\text{Min Value}} \\]\n",
    "\n",
    "   - For each feature (e.g., price, rating, delivery time), calculate the scaled values.\n",
    "\n",
    "4. **Transformed Data:**\n",
    "   - The dataset now contains scaled values for each feature, ensuring that all features are within the [0, 1] range.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider a simplified example with a small dataset:\n",
    "\n",
    "```plaintext\n",
    "Original Data:\n",
    "- Price: $5, $15, $10\n",
    "- Rating: 3.5, 4.8, 4.2\n",
    "- Delivery Time: 20 mins, 40 mins, 30 mins\n",
    "```\n",
    "\n",
    "Applying Min-Max scaling:\n",
    "\n",
    "- For Price:\n",
    "  - \\( \\text{Scaled Price} = \\frac{\\text{Price} - \\text{\\$5}}{\\text{\\$15} - \\text{\\$5}} \\)\n",
    "\n",
    "- For Rating:\n",
    "  - \\( \\text{Scaled Rating} = \\frac{\\text{Rating} - 3.5}{4.8 - 3.5} \\)\n",
    "\n",
    "- For Delivery Time:\n",
    "  - \\( \\text{Scaled Delivery Time} = \\frac{\\text{Delivery Time} - 20}{40 - 20} \\)\n",
    "\n",
    "The transformed dataset would have scaled values for each feature, making them suitable for use in a recommendation system where features with different scales could impact the recommendation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8053745-94c8-400b-874e-8be7029be201",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12da9ff8-48b9-4640-9a7f-6adce43f0dc7",
   "metadata": {},
   "source": [
    "**Using PCA to Reduce Dimensionality in Predicting Stock Prices:**\n",
    "\n",
    "In the context of building a model to predict stock prices with a dataset containing numerous features (company financial data and market trends), Principal Component Analysis (PCA) can be employed to reduce the dimensionality of the dataset. The goal is to capture the most significant information in the data while reducing the number of features, which can improve model training and performance.\n",
    "\n",
    "Here's how you would use PCA for dimensionality reduction in predicting stock prices:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Standardize the dataset by subtracting the mean and dividing by the standard deviation for each feature. This step ensures that all features have a similar scale.\n",
    "\n",
    "2. **Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized dataset. The covariance matrix represents the relationships between different features.\n",
    "\n",
    "3. **Eigendecomposition:**\n",
    "   - Perform eigendecomposition on the covariance matrix to obtain eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Sort Eigenvalues:**\n",
    "   - Sort the eigenvalues in descending order. The eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "5. **Choose the Number of Principal Components:**\n",
    "   - Determine the number of principal components (eigenvectors) to retain. You can choose a number based on the explained variance or a specific percentage of total variance.\n",
    "\n",
    "6. **Projection:**\n",
    "   - Project the original dataset onto the selected principal components to obtain the reduced-dimensional representation.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's assume the original dataset has features such as revenue, earnings, market trends, and more. After applying PCA:\n",
    "\n",
    "- **Original Data:**\n",
    "  - Features: Revenue, Earnings, Market Trends, ...\n",
    "\n",
    "- **Reduced Data:**\n",
    "  - Features: Principal Component 1, Principal Component 2, ...\n",
    "\n",
    "The reduced dataset contains fewer features (principal components) that capture the most significant variance in the original data. These principal components can be used as input features for training a machine learning model to predict stock prices.\n",
    "\n",
    "**Benefits of PCA in Predicting Stock Prices:**\n",
    "\n",
    "1. **Dimensionality Reduction:**\n",
    "   - Reducing the number of features helps avoid the curse of dimensionality and can improve model generalization.\n",
    "\n",
    "2. **Elimination of Redundancy:**\n",
    "   - PCA identifies and retains the most important features while eliminating redundant information.\n",
    "\n",
    "3. **Improved Model Training:**\n",
    "   - Training models with a reduced set of features can lead to faster training times and improved computational efficiency.\n",
    "\n",
    "4. **Noise Reduction:**\n",
    "   - By focusing on principal components with higher eigenvalues, PCA helps filter out noise in the dataset.\n",
    "\n",
    "It's important to note that the choice of the number of principal components should be based on a trade-off between explained variance and the desired level of dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f4858-a22b-451b-9b99-e9b3356c6aa1",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb2f5f61-701c-4b09-a689-4e9a1600cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff8685b0-51b5-4c46-bed7-05ff16ca4612",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdbbef7-13c5-443d-8d57-673509d051a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[1, 5, 10, 15, 20]\n",
    "scaler.fit_transform([l])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da273d2-6714-432a-84dd-280b189d3f16",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978b38cf-5cd6-4e9d-baa7-45a19251800f",
   "metadata": {},
   "source": [
    "The decision on how many principal components to retain in PCA involves considering the explained variance and the trade-off between dimensionality reduction and retaining significant information. Here's a general approach to decide the number of principal components:\n",
    "\n",
    "Calculate Explained Variance:\n",
    "\n",
    "After performing PCA, the eigenvalues represent the variance explained by each principal component. Calculate the percentage of total variance explained by each principal component.\n",
    "Cumulative Explained Variance:\n",
    "\n",
    "Calculate the cumulative explained variance as you go through the sorted eigenvalues. This will give you an idea of how much total variance is retained as you include more principal components.\n",
    "Threshold for Retention:\n",
    "\n",
    "Set a threshold for the minimum acceptable cumulative explained variance. This threshold is typically chosen based on the desired level of information retention. Common choices include 90%, 95%, or 99%.\n",
    "Choose the Number of Components:\n",
    "\n",
    "Choose the number of principal components that achieve or exceed the desired cumulative explained variance threshold.\n",
    "Visualization (Optional):\n",
    "\n",
    "Optionally, visualize the cumulative explained variance graph to visually inspect the point of diminishing returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b80dde-d7b0-4e25-ba75-21ab94dc578c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
