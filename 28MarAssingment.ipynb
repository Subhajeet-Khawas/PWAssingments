{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb0b4b9-3319-4caa-8b9e-4afaefbfd633",
   "metadata": {},
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a11ab-dfc3-481a-bd1b-1ed36847d322",
   "metadata": {},
   "source": [
    "**Ridge Regression:**\n",
    "\n",
    "Ridge regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique that extends ordinary least squares (OLS) regression by adding a penalty term to the linear regression objective function. This penalty term discourages large coefficients in the model, helping to prevent overfitting and mitigate the impact of multicollinearity.\n",
    "\n",
    "**Objective Function of Ridge Regression:**\n",
    "\\[ \\text{Minimize} \\left\\{ \\text{Sum of Squared Residuals} + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right\\} \\]\n",
    "\n",
    "- **Sum of Squared Residuals:** This is the same as the OLS objective, aiming to minimize the difference between the predicted and actual values.\n",
    "\n",
    "- **Penalty Term (\\(\\lambda \\sum_{j=1}^{p} \\beta_j^2\\)):** The additional term penalizes the sum of the squared values of the coefficients (\\(\\beta_j\\)). The hyperparameter \\(\\lambda\\) controls the strength of the penalty. The larger the \\(\\lambda\\), the stronger the regularization, and the more the coefficients are pushed towards zero.\n",
    "\n",
    "**Key Differences from Ordinary Least Squares (OLS) Regression:**\n",
    "\n",
    "1. **Regularization Penalty:**\n",
    "   - Ridge regression introduces a regularization penalty term that is absent in ordinary least squares. This penalty discourages large coefficients, preventing them from becoming too influential in the model.\n",
    "\n",
    "2. **Coefficient Shrinkage:**\n",
    "   - Ridge regression tends to shrink the coefficients towards zero, but it rarely sets them exactly to zero. This is in contrast to Lasso regression, which has a feature selection property and can set some coefficients exactly to zero.\n",
    "\n",
    "3. **Handling Multicollinearity:**\n",
    "   - Ridge regression is particularly useful when dealing with multicollinearity, where predictor variables are highly correlated. It stabilizes the coefficient estimates, preventing them from fluctuating wildly in the presence of multicollinearity.\n",
    "\n",
    "4. **Sensitivity to Scale:**\n",
    "   - Ridge regression is sensitive to the scale of the features. Standardizing or normalizing the features is often recommended before applying Ridge regression to ensure fair treatment of all features.\n",
    "\n",
    "5. **Optimal for All Features:**\n",
    "   - Ridge regression tends to perform well when all features contribute meaningfully to the prediction task. It does not perform automatic feature selection; all features are retained.\n",
    "\n",
    "**When to Use Ridge Regression:**\n",
    "- Ridge regression is suitable when dealing with multicollinearity, preventing overfitting in the presence of highly correlated predictors.\n",
    "- It is effective when all features are expected to contribute to the model, and there is no prior belief that certain features should be excluded.\n",
    "\n",
    "In summary, Ridge regression is a regularization technique that extends ordinary least squares regression by introducing a penalty for large coefficients. It is useful in preventing overfitting, handling multicollinearity, and providing stable estimates of the coefficients. The choice between Ridge and OLS depends on the characteristics of the data and the goals of the modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2ff39-c57f-41b9-bda8-2f2e20c8f26a",
   "metadata": {},
   "source": [
    "Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73cb48-bcfe-4ffb-b290-44f21eeb3363",
   "metadata": {},
   "source": [
    "Ridge regression shares many assumptions with ordinary least squares (OLS) regression, as both are linear regression techniques. The key assumptions of Ridge Regression are:\n",
    "\n",
    "1. **Linearity:**\n",
    "   - Ridge regression assumes a linear relationship between the predictor variables and the response variable. The model aims to capture this linear relationship through the coefficients.\n",
    "\n",
    "2. **Independence of Errors:**\n",
    "   - The errors (residuals) should be independent of each other. The presence of correlation among errors may violate this assumption.\n",
    "\n",
    "3. **Homoscedasticity:**\n",
    "   - The variance of the errors should be constant across all levels of the predictor variables. This assumption ensures that the spread of residuals is consistent, and there are no patterns in the residuals related to the predictor variables.\n",
    "\n",
    "4. **Normality of Residuals:**\n",
    "   - Ridge regression does not require the residuals to be normally distributed. However, normality assumptions may be relevant for making statistical inferences or constructing confidence intervals.\n",
    "\n",
    "5. **No Perfect Multicollinearity:**\n",
    "   - While ridge regression is designed to handle multicollinearity, it assumes that there is no perfect multicollinearity, where one predictor variable is a perfect linear combination of others.\n",
    "\n",
    "6. **No Endogeneity:**\n",
    "   - Ridge regression assumes that there is no endogeneity, meaning that the predictor variables are not correlated with the error term. Endogeneity can lead to biased coefficient estimates.\n",
    "\n",
    "7. **Linear Independence of Predictors:**\n",
    "   - The predictor variables should be linearly independent, and the design matrix (matrix of predictor variables) should have full rank. In the presence of perfect multicollinearity, the design matrix becomes singular, and the regression coefficients cannot be uniquely determined.\n",
    "\n",
    "8. **Scale of Predictors:**\n",
    "   - Ridge regression is sensitive to the scale of predictor variables. Standardizing or normalizing the features is often recommended to ensure fair treatment of all features and prevent a dominance of one variable due to its scale.\n",
    "\n",
    "It's important to note that while ridge regression relaxes the assumption of no multicollinearity, it introduces a regularization term that penalizes large coefficients. This regularization term helps stabilize the coefficient estimates in the presence of correlated predictors.\n",
    "\n",
    "Overall, while ridge regression is more robust to multicollinearity compared to OLS regression, it is still essential to check and satisfy the relevant assumptions for reliable and meaningful results. The appropriateness of ridge regression also depends on the specific characteristics of the data and the goals of the modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935fc83-8e2d-4cdd-b9db-2d04cf986302",
   "metadata": {},
   "source": [
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c621779d-d4a6-4536-847e-57d9ddd19acb",
   "metadata": {},
   "source": [
    "The tuning parameter in Ridge Regression, often denoted as \\(\\lambda\\), controls the strength of the regularization penalty. Selecting an appropriate value for \\(\\lambda\\) is crucial for the performance of the Ridge Regression model. Several methods can be employed to determine the optimal \\(\\lambda\\):\n",
    "\n",
    "1. **Cross-Validation:**\n",
    "   - One of the most common methods for tuning \\(\\lambda\\) is cross-validation. The dataset is split into multiple folds, and the model is trained on subsets of the data, with each fold serving as both a training and validation set. The value of \\(\\lambda\\) that results in the best performance (e.g., minimum mean squared error) on the validation sets is chosen.\n",
    "\n",
    "2. **Grid Search:**\n",
    "   - A grid search involves trying out multiple values of \\(\\lambda\\) and evaluating the model's performance for each value. The optimal \\(\\lambda\\) is then selected based on the best performance observed during the grid search. This method is often combined with cross-validation.\n",
    "\n",
    "3. **Regularization Paths:**\n",
    "   - Some optimization algorithms for Ridge Regression, like coordinate descent, can compute the entire regularization path efficiently. This path shows how the coefficients change for a range of \\(\\lambda\\) values. Techniques like cross-validation can then be applied to select the optimal \\(\\lambda\\) based on the regularization path.\n",
    "\n",
    "4. **Analytical Solutions:**\n",
    "   - In certain cases, analytical solutions exist for finding the optimal \\(\\lambda\\). For example, the generalized cross-validation (GCV) score is a criterion that can be used to determine the optimal \\(\\lambda\\) without explicitly cross-validating over multiple folds.\n",
    "\n",
    "5. **Information Criteria:**\n",
    "   - Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used for model selection. These criteria balance model fit with model complexity and can guide the choice of \\(\\lambda\\).\n",
    "\n",
    "6. **Heuristic Rules:**\n",
    "   - Some practitioners may use heuristic rules or domain knowledge to select \\(\\lambda\\). For example, choosing a value of \\(\\lambda\\) that shrinks coefficients sufficiently without overly penalizing them.\n",
    "\n",
    "It's important to note that the choice of the method for selecting \\(\\lambda\\) depends on factors such as the size of the dataset, computational resources, and the specific goals of the analysis. Cross-validation is a widely used and robust approach, providing a good balance between bias and variance in the model selection process.\n",
    "\n",
    "In practice, tools like scikit-learn in Python provide convenient functions for implementing cross-validation and grid search for hyperparameter tuning in Ridge Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4435da-4069-47c4-ab38-9a775a48f84b",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a71a5-7e09-455b-a2af-2745cd036124",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for feature selection, although it is not as effective in this regard as Lasso Regression. Ridge Regression introduces a regularization term that penalizes the sum of squared coefficients, but it rarely sets any coefficients exactly to zero. However, the penalty term in Ridge Regression can still shrink the coefficients towards zero, and as a result, it may reduce the impact of less important features.\n",
    "\n",
    "The key idea is that as the regularization parameter (\\(\\lambda\\)) in Ridge Regression increases, the penalty for large coefficients becomes stronger. This leads Ridge Regression to favor simpler models with smaller coefficients. While Ridge Regression does not typically result in exact feature selection, it can make the contributions of less important features negligible.\n",
    "\n",
    "Here are the steps to use Ridge Regression for feature selection:\n",
    "\n",
    "1. **Standardize the Features:**\n",
    "   - Before applying Ridge Regression, it's essential to standardize or normalize the features to ensure that all features are on a comparable scale. This step is crucial because Ridge Regression is sensitive to the scale of the features.\n",
    "\n",
    "2. **Choose a Range of \\(\\lambda\\):**\n",
    "   - Select a range of \\(\\lambda\\) values to test. Typically, these values are chosen in a logarithmic or geometric sequence to cover a broad spectrum of regularization strengths.\n",
    "\n",
    "3. **Apply Ridge Regression with Cross-Validation:**\n",
    "   - Use cross-validation to train Ridge Regression models with different \\(\\lambda\\) values. Evaluate the performance of each model using an appropriate metric (e.g., mean squared error) on the validation set.\n",
    "\n",
    "4. **Select the Optimal \\(\\lambda\\):**\n",
    "   - Choose the \\(\\lambda\\) value that provides the best trade-off between model complexity and performance on the validation set. This is often done using techniques like k-fold cross-validation.\n",
    "\n",
    "5. **Analyze Coefficients:**\n",
    "   - Examine the coefficients of the Ridge Regression model. As \\(\\lambda\\) increases, some coefficients may shrink towards zero. While they may not reach zero, their contributions become negligible.\n",
    "\n",
    "6. **Feature Importance Ranking:**\n",
    "   - Rank the features based on the magnitude of their coefficients in the Ridge Regression model. Features with smaller coefficients are considered less important.\n",
    "\n",
    "It's important to note that Ridge Regression is generally not as effective for feature selection as Lasso Regression, which has a more pronounced feature selection property by setting some coefficients exactly to zero. The choice between Ridge and Lasso depends on the specific goals and characteristics of the data. If feature selection is a primary objective, and sparsity in the model is desired, Lasso Regression may be a more suitable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e253d794-52d9-4b1f-9bf5-26706ac48b57",
   "metadata": {},
   "source": [
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74480759-4c74-4e44-b325-799c31563b59",
   "metadata": {},
   "source": [
    "Ridge Regression is particularly useful in the presence of multicollinearity, a situation where predictor variables are highly correlated with each other. Multicollinearity can cause issues in ordinary least squares (OLS) regression by making the coefficient estimates highly sensitive to small changes in the data, leading to instability and unreliable results. Ridge Regression addresses this problem by introducing a regularization term that stabilizes the coefficient estimates.\n",
    "\n",
    "Here's how Ridge Regression performs in the presence of multicollinearity:\n",
    "\n",
    "1. **Stabilization of Coefficient Estimates:**\n",
    "   - Ridge Regression adds a penalty term proportional to the sum of squared coefficients to the ordinary least squares objective function. This penalty term helps to control the size of the coefficients, preventing them from becoming too large. As a result, Ridge Regression stabilizes the coefficient estimates, making them less sensitive to multicollinearity.\n",
    "\n",
    "2. **Shrinkage of Coefficients:**\n",
    "   - The penalty term in Ridge Regression encourages the model to shrink the coefficients towards zero. While it rarely sets any coefficients exactly to zero, it does make the coefficients smaller, mitigating the impact of highly correlated predictors.\n",
    "\n",
    "3. **Effective Handling of Near-Collinear Features:**\n",
    "   - Ridge Regression is effective not only when there is perfect multicollinearity (where one predictor is a perfect linear combination of others) but also in cases of near-collinearity. It can allocate the impact of correlated predictors more evenly across them.\n",
    "\n",
    "4. **Controlled Trade-off Between Bias and Variance:**\n",
    "   - By introducing the regularization term, Ridge Regression allows for a controlled trade-off between bias and variance. It reduces the variance in the coefficient estimates at the expense of introducing a small amount of bias. This trade-off is beneficial in the presence of multicollinearity.\n",
    "\n",
    "5. **Applicability to High-Dimensional Data:**\n",
    "   - Ridge Regression is particularly useful when dealing with high-dimensional datasets where the number of predictors is comparable to or exceeds the number of observations. In such cases, multicollinearity issues can be more pronounced, and Ridge Regression helps to provide stable and reliable results.\n",
    "\n",
    "It's important to note that while Ridge Regression is effective in handling multicollinearity, it does not perform variable selection in the sense of setting some coefficients exactly to zero. If variable selection is a primary concern, Lasso Regression (L1 regularization) might be more appropriate, as it has a more pronounced feature selection property.\n",
    "\n",
    "In summary, Ridge Regression is a valuable tool for addressing multicollinearity in regression models. It provides stability to the coefficient estimates and allows for a controlled trade-off between bias and variance, making it particularly useful in situations where predictors are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb6e21-c692-4707-ac4d-c866799776d2",
   "metadata": {},
   "source": [
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c78f2-126c-4425-ab60-668e06354e39",
   "metadata": {},
   "source": [
    "Ridge Regression is primarily designed for handling continuous independent variables, and it is an extension of ordinary least squares (OLS) regression to mitigate issues like multicollinearity and overfitting. It assumes a linear relationship between the predictors and the response variable. As such, Ridge Regression is typically applied to problems where the independent variables are continuous.\n",
    "\n",
    "However, Ridge Regression can be adapted to handle categorical variables through appropriate encoding techniques. The most common approach is to use dummy encoding or one-hot encoding to represent categorical variables as binary (0/1) indicators. Each category of a categorical variable is represented by a binary column, and Ridge Regression can be applied to the combined set of continuous and encoded categorical variables.\n",
    "\n",
    "Here's a basic overview of how Ridge Regression can be used with both continuous and categorical variables:\n",
    "\n",
    "1. **Continuous Variables:**\n",
    "   - Continuous variables can be included in the regression model as they are.\n",
    "\n",
    "2. **Categorical Variables:**\n",
    "   - Categorical variables need to be encoded. Common encoding methods include:\n",
    "     - **Dummy Encoding:** Create binary (0/1) indicator variables for each category.\n",
    "     - **One-Hot Encoding:** Similar to dummy encoding, but only one indicator variable is \"hot\" (1) at a time.\n",
    "\n",
    "3. **Standardization or Normalization:**\n",
    "   - Before applying Ridge Regression, it's often recommended to standardize or normalize the variables, especially if they are on different scales. This ensures that the regularization term is applied fairly to all variables.\n",
    "\n",
    "4. **Apply Ridge Regression:**\n",
    "   - Once the data is prepared with continuous and encoded categorical variables, Ridge Regression can be applied to estimate the coefficients.\n",
    "\n",
    "It's important to note that while Ridge Regression can handle encoded categorical variables, it doesn't inherently provide special treatment for them. Other techniques, like regularization methods designed specifically for handling categorical variables, might be considered in certain situations.\n",
    "\n",
    "Additionally, when working with high-cardinality categorical variables (those with many unique categories), the introduction of dummy variables can significantly increase the dimensionality of the data, and careful consideration should be given to potential issues related to multicollinearity and computational efficiency.\n",
    "\n",
    "In summary, Ridge Regression can be used with a combination of continuous and encoded categorical variables, but it requires appropriate preprocessing steps such as encoding and standardization to handle both types of variables effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f282307-96a7-4f2c-9fa0-3d3dee6cfd6f",
   "metadata": {},
   "source": [
    "Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb6108-aa47-4675-80ae-156f543f5b16",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of Ridge Regression is similar to interpreting the coefficients in ordinary least squares (OLS) regression. However, due to the regularization term in Ridge Regression, there are some nuances to consider. Here are key points to keep in mind when interpreting the coefficients:\n",
    "\n",
    "1. **Magnitude of Coefficients:**\n",
    "   - In Ridge Regression, the penalty term encourages smaller coefficient values. Therefore, the magnitudes of the coefficients in Ridge Regression tend to be smaller than those in OLS regression. The larger the regularization parameter (\\(\\lambda\\)), the more the coefficients are shrunk towards zero.\n",
    "\n",
    "2. **Direction of Coefficients:**\n",
    "   - The sign of the coefficients indicates the direction of the relationship between each predictor variable and the response variable. A positive coefficient implies a positive relationship, while a negative coefficient implies a negative relationship.\n",
    "\n",
    "3. **Relative Importance:**\n",
    "   - Comparing the magnitudes of coefficients can provide insights into the relative importance of different predictors. However, it's essential to consider the scale of the predictors, as Ridge Regression is sensitive to the scale of the variables.\n",
    "\n",
    "4. **Not Suitable for Variable Selection:**\n",
    "   - Unlike some other regression techniques (e.g., Lasso Regression), Ridge Regression does not set coefficients exactly to zero. All variables are retained in the model, but their contributions are penalized, and some may be shrunk close to zero.\n",
    "\n",
    "5. **Interaction Effects:**\n",
    "   - If interaction terms are included in the model, the interpretation becomes more complex. The impact of one variable on the response depends on the values of other interacting variables.\n",
    "\n",
    "6. **Standardization for Comparisons:**\n",
    "   - To facilitate fair comparisons between coefficients, it's common to standardize the predictor variables before applying Ridge Regression. Standardization involves subtracting the mean and dividing by the standard deviation for each variable.\n",
    "\n",
    "7. **Impact of Regularization Parameter (\\(\\lambda\\)):**\n",
    "   - The regularization parameter (\\(\\lambda\\)) controls the strength of the penalty term. As \\(\\lambda\\) increases, the coefficients are shrunk more, and their magnitudes decrease. The choice of \\(\\lambda\\) should be based on cross-validation or other model selection techniques.\n",
    "\n",
    "8. **Interpretation as Regularized OLS Coefficients:**\n",
    "   - One way to interpret the coefficients in Ridge Regression is to view them as regularized versions of OLS coefficients. The Ridge coefficients are influenced by both the OLS estimates and the penalty term.\n",
    "\n",
    "In summary, while the basic principles of interpreting coefficients apply to Ridge Regression, the regularization term introduces a shrinkage effect. The coefficients represent the impact of predictor variables on the response, considering both their relationships and the regularization-induced penalties. Understanding the balance between bias and variance in Ridge Regression is crucial for appropriate interpretation and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd7465-9fb2-4463-b3f6-08c2236cafe0",
   "metadata": {},
   "source": [
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf648c-c303-43d5-998c-8eed881eb02b",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis, but its application to time-series data requires careful consideration of the temporal nature of the data. Time-series data typically exhibits autocorrelation, where observations at one time point are correlated with observations at nearby time points. Ridge Regression can be adapted for time-series analysis, but it's important to address the temporal dependencies appropriately.\n",
    "\n",
    "Here's how Ridge Regression can be applied to time-series data:\n",
    "\n",
    "1. **Stationarity:**\n",
    "   - Ensure that the time series is stationary. Stationarity implies that the statistical properties of the time series, such as mean and variance, do not change over time. If the time series is not stationary, transformations or differencing may be applied to achieve stationarity.\n",
    "\n",
    "2. **Feature Engineering:**\n",
    "   - Create lag features: Introduce lagged values of the target variable or other relevant predictors as features. This accounts for the autocorrelation inherent in time-series data.\n",
    "\n",
    "3. **Train-Test Split:**\n",
    "   - Split the time series into training and testing sets. Ensure that the training set consists of earlier time points, and the testing set contains later time points.\n",
    "\n",
    "4. **Standardization:**\n",
    "   - Standardize the features if necessary. Ridge Regression is sensitive to the scale of the variables, so standardization helps ensure fair treatment of features.\n",
    "\n",
    "5. **Apply Ridge Regression:**\n",
    "   - Train the Ridge Regression model on the training set using lagged values and other relevant predictors. The regularization parameter (\\(\\lambda\\)) should be selected based on cross-validation or other model selection techniques.\n",
    "\n",
    "6. **Model Evaluation:**\n",
    "   - Evaluate the model's performance on the testing set using appropriate time-series metrics. Common metrics include Mean Squared Error (MSE), Mean Absolute Error (MAE), or others suitable for time-series forecasting.\n",
    "\n",
    "7. **Considerations for Autocorrelation:**\n",
    "   - Ridge Regression does not explicitly account for the temporal structure of time series. If autocorrelation is a significant concern, other time-series models, such as autoregressive integrated moving average (ARIMA) or seasonal decomposition of time series (STL), may be more suitable.\n",
    "\n",
    "8. **Dynamic Prediction:**\n",
    "   - In time-series analysis, dynamic prediction involves using actual observations for predictions at each step rather than predicted values. This allows the model to adapt to changes over time.\n",
    "\n",
    "It's important to note that Ridge Regression is just one of many approaches for time-series analysis, and its effectiveness depends on the characteristics of the data. In cases where the temporal structure is complex and traditional linear models may not capture it adequately, more specialized time-series models or machine learning techniques designed for time-series forecasting may be considered.\n",
    "\n",
    "In summary, Ridge Regression can be applied to time-series data with appropriate feature engineering and consideration of the temporal structure. It provides a regularization mechanism that can be valuable in situations where multicollinearity or overfitting is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f240b9d-bc92-40a1-92ee-e3eb8e92f948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
